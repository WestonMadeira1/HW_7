{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNW2XK8uR2p49h35Q/s5Qho",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WestonMadeira1/HW_7/blob/main/HW7_P1A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FqFA15YBkPaa",
        "outputId": "d283bb48-0574-4374-c9de-94a091332bf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 15, 15, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                65600     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 122570 (478.79 KB)\n",
            "Trainable params: 122570 (478.79 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/300\n",
            "1563/1563 - 68s - loss: 1.4945 - accuracy: 0.4569 - val_loss: 1.2362 - val_accuracy: 0.5519 - 68s/epoch - 43ms/step\n",
            "Epoch 2/300\n",
            "1563/1563 - 66s - loss: 1.1436 - accuracy: 0.5946 - val_loss: 1.1139 - val_accuracy: 0.6029 - 66s/epoch - 42ms/step\n",
            "Epoch 3/300\n",
            "1563/1563 - 67s - loss: 1.0025 - accuracy: 0.6470 - val_loss: 0.9854 - val_accuracy: 0.6539 - 67s/epoch - 43ms/step\n",
            "Epoch 4/300\n",
            "1563/1563 - 66s - loss: 0.9109 - accuracy: 0.6796 - val_loss: 0.9553 - val_accuracy: 0.6724 - 66s/epoch - 42ms/step\n",
            "Epoch 5/300\n",
            "1563/1563 - 64s - loss: 0.8404 - accuracy: 0.7051 - val_loss: 0.8960 - val_accuracy: 0.6926 - 64s/epoch - 41ms/step\n",
            "Epoch 6/300\n",
            "1563/1563 - 66s - loss: 0.7891 - accuracy: 0.7247 - val_loss: 0.8971 - val_accuracy: 0.6926 - 66s/epoch - 42ms/step\n",
            "Epoch 7/300\n",
            "1563/1563 - 65s - loss: 0.7410 - accuracy: 0.7405 - val_loss: 0.8805 - val_accuracy: 0.7031 - 65s/epoch - 41ms/step\n",
            "Epoch 8/300\n",
            "1563/1563 - 71s - loss: 0.6970 - accuracy: 0.7549 - val_loss: 0.8695 - val_accuracy: 0.7081 - 71s/epoch - 45ms/step\n",
            "Epoch 9/300\n",
            "1563/1563 - 66s - loss: 0.6588 - accuracy: 0.7691 - val_loss: 0.9047 - val_accuracy: 0.7026 - 66s/epoch - 42ms/step\n",
            "Epoch 10/300\n",
            "1563/1563 - 66s - loss: 0.6234 - accuracy: 0.7816 - val_loss: 0.8909 - val_accuracy: 0.7074 - 66s/epoch - 42ms/step\n",
            "Epoch 11/300\n",
            "1563/1563 - 67s - loss: 0.5958 - accuracy: 0.7904 - val_loss: 0.8870 - val_accuracy: 0.7136 - 67s/epoch - 43ms/step\n",
            "Epoch 12/300\n",
            "1563/1563 - 66s - loss: 0.5629 - accuracy: 0.8023 - val_loss: 0.9515 - val_accuracy: 0.6977 - 66s/epoch - 42ms/step\n",
            "Epoch 13/300\n",
            "1563/1563 - 65s - loss: 0.5352 - accuracy: 0.8093 - val_loss: 0.9326 - val_accuracy: 0.7086 - 65s/epoch - 41ms/step\n",
            "Epoch 14/300\n",
            "1563/1563 - 65s - loss: 0.5107 - accuracy: 0.8184 - val_loss: 0.9230 - val_accuracy: 0.7110 - 65s/epoch - 41ms/step\n",
            "Epoch 15/300\n",
            "1563/1563 - 68s - loss: 0.4798 - accuracy: 0.8294 - val_loss: 0.9876 - val_accuracy: 0.7042 - 68s/epoch - 43ms/step\n",
            "Epoch 16/300\n",
            "1563/1563 - 69s - loss: 0.4524 - accuracy: 0.8390 - val_loss: 0.9801 - val_accuracy: 0.7046 - 69s/epoch - 44ms/step\n",
            "Epoch 17/300\n",
            "1563/1563 - 66s - loss: 0.4319 - accuracy: 0.8470 - val_loss: 0.9778 - val_accuracy: 0.7124 - 66s/epoch - 42ms/step\n",
            "Epoch 18/300\n",
            "1563/1563 - 67s - loss: 0.4129 - accuracy: 0.8524 - val_loss: 1.0124 - val_accuracy: 0.7135 - 67s/epoch - 43ms/step\n",
            "Epoch 19/300\n",
            "1563/1563 - 64s - loss: 0.3872 - accuracy: 0.8611 - val_loss: 1.1004 - val_accuracy: 0.6991 - 64s/epoch - 41ms/step\n",
            "Epoch 20/300\n",
            "1563/1563 - 67s - loss: 0.3701 - accuracy: 0.8675 - val_loss: 1.1220 - val_accuracy: 0.6947 - 67s/epoch - 43ms/step\n",
            "Epoch 21/300\n",
            "1563/1563 - 66s - loss: 0.3488 - accuracy: 0.8748 - val_loss: 1.1591 - val_accuracy: 0.6902 - 66s/epoch - 42ms/step\n",
            "Epoch 22/300\n",
            "1563/1563 - 66s - loss: 0.3350 - accuracy: 0.8792 - val_loss: 1.1828 - val_accuracy: 0.6982 - 66s/epoch - 42ms/step\n",
            "Epoch 23/300\n",
            "1563/1563 - 66s - loss: 0.3122 - accuracy: 0.8879 - val_loss: 1.2744 - val_accuracy: 0.6900 - 66s/epoch - 42ms/step\n",
            "Epoch 24/300\n",
            "1563/1563 - 67s - loss: 0.2911 - accuracy: 0.8949 - val_loss: 1.3911 - val_accuracy: 0.6939 - 67s/epoch - 43ms/step\n",
            "Epoch 25/300\n",
            "1563/1563 - 67s - loss: 0.2826 - accuracy: 0.8965 - val_loss: 1.3143 - val_accuracy: 0.6950 - 67s/epoch - 43ms/step\n",
            "Epoch 26/300\n",
            "1563/1563 - 67s - loss: 0.2651 - accuracy: 0.9039 - val_loss: 1.3976 - val_accuracy: 0.6968 - 67s/epoch - 43ms/step\n",
            "Epoch 27/300\n",
            "1563/1563 - 64s - loss: 0.2532 - accuracy: 0.9080 - val_loss: 1.4633 - val_accuracy: 0.6875 - 64s/epoch - 41ms/step\n",
            "Epoch 28/300\n",
            "1563/1563 - 66s - loss: 0.2399 - accuracy: 0.9120 - val_loss: 1.4674 - val_accuracy: 0.6921 - 66s/epoch - 42ms/step\n",
            "Epoch 29/300\n",
            "1563/1563 - 66s - loss: 0.2274 - accuracy: 0.9187 - val_loss: 1.5520 - val_accuracy: 0.6981 - 66s/epoch - 42ms/step\n",
            "Epoch 30/300\n",
            "1563/1563 - 66s - loss: 0.2170 - accuracy: 0.9212 - val_loss: 1.5979 - val_accuracy: 0.6838 - 66s/epoch - 42ms/step\n",
            "Epoch 31/300\n",
            "1563/1563 - 64s - loss: 0.2160 - accuracy: 0.9227 - val_loss: 1.6221 - val_accuracy: 0.6861 - 64s/epoch - 41ms/step\n",
            "Epoch 32/300\n",
            "1563/1563 - 66s - loss: 0.2008 - accuracy: 0.9268 - val_loss: 1.7682 - val_accuracy: 0.6850 - 66s/epoch - 42ms/step\n",
            "Epoch 33/300\n",
            "1563/1563 - 67s - loss: 0.1968 - accuracy: 0.9281 - val_loss: 1.6935 - val_accuracy: 0.6934 - 67s/epoch - 43ms/step\n",
            "Epoch 34/300\n",
            "1563/1563 - 67s - loss: 0.1840 - accuracy: 0.9340 - val_loss: 1.8511 - val_accuracy: 0.6844 - 67s/epoch - 43ms/step\n",
            "Epoch 35/300\n",
            "1563/1563 - 66s - loss: 0.1854 - accuracy: 0.9328 - val_loss: 1.9722 - val_accuracy: 0.6707 - 66s/epoch - 42ms/step\n",
            "Epoch 36/300\n",
            "1563/1563 - 65s - loss: 0.1705 - accuracy: 0.9399 - val_loss: 1.9362 - val_accuracy: 0.6754 - 65s/epoch - 42ms/step\n",
            "Epoch 37/300\n",
            "1563/1563 - 64s - loss: 0.1694 - accuracy: 0.9394 - val_loss: 2.0438 - val_accuracy: 0.6802 - 64s/epoch - 41ms/step\n",
            "Epoch 38/300\n",
            "1563/1563 - 66s - loss: 0.1688 - accuracy: 0.9391 - val_loss: 2.0102 - val_accuracy: 0.6835 - 66s/epoch - 42ms/step\n",
            "Epoch 39/300\n",
            "1563/1563 - 64s - loss: 0.1556 - accuracy: 0.9439 - val_loss: 2.0283 - val_accuracy: 0.6823 - 64s/epoch - 41ms/step\n",
            "Epoch 40/300\n",
            "1563/1563 - 66s - loss: 0.1592 - accuracy: 0.9434 - val_loss: 2.0568 - val_accuracy: 0.6734 - 66s/epoch - 42ms/step\n",
            "Epoch 41/300\n",
            "1563/1563 - 67s - loss: 0.1487 - accuracy: 0.9475 - val_loss: 2.1059 - val_accuracy: 0.6848 - 67s/epoch - 43ms/step\n",
            "Epoch 42/300\n",
            "1563/1563 - 66s - loss: 0.1501 - accuracy: 0.9467 - val_loss: 2.0811 - val_accuracy: 0.6837 - 66s/epoch - 42ms/step\n",
            "Epoch 43/300\n",
            "1563/1563 - 66s - loss: 0.1457 - accuracy: 0.9479 - val_loss: 2.1989 - val_accuracy: 0.6776 - 66s/epoch - 42ms/step\n",
            "Epoch 44/300\n",
            "1563/1563 - 66s - loss: 0.1458 - accuracy: 0.9482 - val_loss: 2.2852 - val_accuracy: 0.6834 - 66s/epoch - 42ms/step\n",
            "Epoch 45/300\n",
            "1563/1563 - 64s - loss: 0.1441 - accuracy: 0.9488 - val_loss: 2.3036 - val_accuracy: 0.6790 - 64s/epoch - 41ms/step\n",
            "Epoch 46/300\n",
            "1563/1563 - 66s - loss: 0.1337 - accuracy: 0.9514 - val_loss: 2.3485 - val_accuracy: 0.6903 - 66s/epoch - 42ms/step\n",
            "Epoch 47/300\n",
            "1563/1563 - 66s - loss: 0.1372 - accuracy: 0.9525 - val_loss: 2.3556 - val_accuracy: 0.6877 - 66s/epoch - 42ms/step\n",
            "Epoch 48/300\n",
            "1563/1563 - 66s - loss: 0.1322 - accuracy: 0.9533 - val_loss: 2.3198 - val_accuracy: 0.6833 - 66s/epoch - 42ms/step\n",
            "Epoch 49/300\n",
            "1563/1563 - 67s - loss: 0.1285 - accuracy: 0.9544 - val_loss: 2.5035 - val_accuracy: 0.6689 - 67s/epoch - 43ms/step\n",
            "Epoch 50/300\n",
            "1563/1563 - 66s - loss: 0.1262 - accuracy: 0.9568 - val_loss: 2.4850 - val_accuracy: 0.6825 - 66s/epoch - 42ms/step\n",
            "Epoch 51/300\n",
            "1563/1563 - 64s - loss: 0.1294 - accuracy: 0.9549 - val_loss: 2.4082 - val_accuracy: 0.6900 - 64s/epoch - 41ms/step\n",
            "Epoch 52/300\n",
            "1563/1563 - 66s - loss: 0.1191 - accuracy: 0.9574 - val_loss: 2.4907 - val_accuracy: 0.6848 - 66s/epoch - 42ms/step\n",
            "Epoch 53/300\n",
            "1563/1563 - 64s - loss: 0.1191 - accuracy: 0.9584 - val_loss: 2.5160 - val_accuracy: 0.6828 - 64s/epoch - 41ms/step\n",
            "Epoch 54/300\n",
            "1563/1563 - 64s - loss: 0.1251 - accuracy: 0.9564 - val_loss: 2.4977 - val_accuracy: 0.6845 - 64s/epoch - 41ms/step\n",
            "Epoch 55/300\n",
            "1563/1563 - 66s - loss: 0.1263 - accuracy: 0.9569 - val_loss: 2.6258 - val_accuracy: 0.6802 - 66s/epoch - 42ms/step\n",
            "Epoch 56/300\n",
            "1563/1563 - 63s - loss: 0.1127 - accuracy: 0.9612 - val_loss: 2.7151 - val_accuracy: 0.6745 - 63s/epoch - 40ms/step\n",
            "Epoch 57/300\n",
            "1563/1563 - 64s - loss: 0.1171 - accuracy: 0.9584 - val_loss: 2.7255 - val_accuracy: 0.6758 - 64s/epoch - 41ms/step\n",
            "Epoch 58/300\n",
            "1563/1563 - 62s - loss: 0.1154 - accuracy: 0.9599 - val_loss: 2.7188 - val_accuracy: 0.6777 - 62s/epoch - 40ms/step\n",
            "Epoch 59/300\n",
            "1563/1563 - 62s - loss: 0.1144 - accuracy: 0.9613 - val_loss: 2.6829 - val_accuracy: 0.6829 - 62s/epoch - 40ms/step\n",
            "Epoch 60/300\n",
            "1563/1563 - 61s - loss: 0.1100 - accuracy: 0.9621 - val_loss: 2.9122 - val_accuracy: 0.6751 - 61s/epoch - 39ms/step\n",
            "Epoch 61/300\n",
            "1563/1563 - 62s - loss: 0.1113 - accuracy: 0.9621 - val_loss: 2.7390 - val_accuracy: 0.6779 - 62s/epoch - 40ms/step\n",
            "Epoch 62/300\n",
            "1563/1563 - 63s - loss: 0.1135 - accuracy: 0.9608 - val_loss: 2.8144 - val_accuracy: 0.6801 - 63s/epoch - 41ms/step\n",
            "Epoch 63/300\n",
            "1563/1563 - 62s - loss: 0.1058 - accuracy: 0.9649 - val_loss: 2.8718 - val_accuracy: 0.6718 - 62s/epoch - 39ms/step\n",
            "Epoch 64/300\n",
            "1563/1563 - 63s - loss: 0.1184 - accuracy: 0.9599 - val_loss: 2.8528 - val_accuracy: 0.6769 - 63s/epoch - 40ms/step\n",
            "Epoch 65/300\n",
            "1563/1563 - 62s - loss: 0.0941 - accuracy: 0.9678 - val_loss: 3.0618 - val_accuracy: 0.6656 - 62s/epoch - 40ms/step\n",
            "Epoch 66/300\n",
            "1563/1563 - 62s - loss: 0.1079 - accuracy: 0.9636 - val_loss: 2.9549 - val_accuracy: 0.6792 - 62s/epoch - 40ms/step\n",
            "Epoch 67/300\n",
            "1563/1563 - 62s - loss: 0.1084 - accuracy: 0.9630 - val_loss: 3.0599 - val_accuracy: 0.6678 - 62s/epoch - 40ms/step\n",
            "Epoch 68/300\n",
            "1563/1563 - 62s - loss: 0.1009 - accuracy: 0.9665 - val_loss: 3.0225 - val_accuracy: 0.6767 - 62s/epoch - 40ms/step\n",
            "Epoch 69/300\n",
            "1563/1563 - 63s - loss: 0.1042 - accuracy: 0.9641 - val_loss: 2.9792 - val_accuracy: 0.6688 - 63s/epoch - 40ms/step\n",
            "Epoch 70/300\n",
            "1563/1563 - 63s - loss: 0.0992 - accuracy: 0.9663 - val_loss: 3.0755 - val_accuracy: 0.6727 - 63s/epoch - 40ms/step\n",
            "Epoch 71/300\n",
            "1563/1563 - 62s - loss: 0.1003 - accuracy: 0.9665 - val_loss: 2.9994 - val_accuracy: 0.6787 - 62s/epoch - 40ms/step\n",
            "Epoch 72/300\n",
            "1563/1563 - 62s - loss: 0.0962 - accuracy: 0.9674 - val_loss: 3.0992 - val_accuracy: 0.6771 - 62s/epoch - 40ms/step\n",
            "Epoch 73/300\n",
            "1563/1563 - 64s - loss: 0.1056 - accuracy: 0.9644 - val_loss: 3.0377 - val_accuracy: 0.6809 - 64s/epoch - 41ms/step\n",
            "Epoch 74/300\n",
            "1563/1563 - 62s - loss: 0.0954 - accuracy: 0.9688 - val_loss: 3.1239 - val_accuracy: 0.6812 - 62s/epoch - 40ms/step\n",
            "Epoch 75/300\n",
            "1563/1563 - 62s - loss: 0.0885 - accuracy: 0.9697 - val_loss: 3.1112 - val_accuracy: 0.6774 - 62s/epoch - 40ms/step\n",
            "Epoch 76/300\n",
            "1563/1563 - 61s - loss: 0.1058 - accuracy: 0.9644 - val_loss: 3.1096 - val_accuracy: 0.6775 - 61s/epoch - 39ms/step\n",
            "Epoch 77/300\n",
            "1563/1563 - 64s - loss: 0.0985 - accuracy: 0.9668 - val_loss: 3.2071 - val_accuracy: 0.6782 - 64s/epoch - 41ms/step\n",
            "Epoch 78/300\n",
            "1563/1563 - 62s - loss: 0.0958 - accuracy: 0.9676 - val_loss: 3.1298 - val_accuracy: 0.6830 - 62s/epoch - 40ms/step\n",
            "Epoch 79/300\n",
            "1563/1563 - 62s - loss: 0.0904 - accuracy: 0.9701 - val_loss: 3.1646 - val_accuracy: 0.6771 - 62s/epoch - 40ms/step\n",
            "Epoch 80/300\n",
            "1563/1563 - 62s - loss: 0.0968 - accuracy: 0.9684 - val_loss: 3.1316 - val_accuracy: 0.6791 - 62s/epoch - 40ms/step\n",
            "Epoch 81/300\n",
            "1563/1563 - 64s - loss: 0.0849 - accuracy: 0.9727 - val_loss: 3.2053 - val_accuracy: 0.6757 - 64s/epoch - 41ms/step\n",
            "Epoch 82/300\n",
            "1563/1563 - 61s - loss: 0.0988 - accuracy: 0.9671 - val_loss: 3.2524 - val_accuracy: 0.6780 - 61s/epoch - 39ms/step\n",
            "Epoch 83/300\n",
            "1563/1563 - 62s - loss: 0.0889 - accuracy: 0.9698 - val_loss: 3.2793 - val_accuracy: 0.6798 - 62s/epoch - 40ms/step\n",
            "Epoch 84/300\n",
            "1563/1563 - 63s - loss: 0.0942 - accuracy: 0.9684 - val_loss: 3.2642 - val_accuracy: 0.6783 - 63s/epoch - 40ms/step\n",
            "Epoch 85/300\n",
            "1563/1563 - 62s - loss: 0.0866 - accuracy: 0.9716 - val_loss: 3.2913 - val_accuracy: 0.6779 - 62s/epoch - 40ms/step\n",
            "Epoch 86/300\n",
            "1563/1563 - 62s - loss: 0.0884 - accuracy: 0.9709 - val_loss: 3.2295 - val_accuracy: 0.6845 - 62s/epoch - 40ms/step\n",
            "Epoch 87/300\n",
            "1563/1563 - 61s - loss: 0.0980 - accuracy: 0.9681 - val_loss: 3.3461 - val_accuracy: 0.6797 - 61s/epoch - 39ms/step\n",
            "Epoch 88/300\n",
            "1563/1563 - 63s - loss: 0.0853 - accuracy: 0.9709 - val_loss: 3.3570 - val_accuracy: 0.6682 - 63s/epoch - 41ms/step\n",
            "Epoch 89/300\n",
            "1563/1563 - 62s - loss: 0.0953 - accuracy: 0.9686 - val_loss: 3.4746 - val_accuracy: 0.6731 - 62s/epoch - 40ms/step\n",
            "Epoch 90/300\n",
            "1563/1563 - 62s - loss: 0.0841 - accuracy: 0.9724 - val_loss: 3.4742 - val_accuracy: 0.6714 - 62s/epoch - 40ms/step\n",
            "Epoch 91/300\n",
            "1563/1563 - 62s - loss: 0.0927 - accuracy: 0.9705 - val_loss: 3.5599 - val_accuracy: 0.6596 - 62s/epoch - 40ms/step\n",
            "Epoch 92/300\n",
            "1563/1563 - 62s - loss: 0.0885 - accuracy: 0.9712 - val_loss: 3.4536 - val_accuracy: 0.6738 - 62s/epoch - 40ms/step\n",
            "Epoch 93/300\n",
            "1563/1563 - 61s - loss: 0.0841 - accuracy: 0.9732 - val_loss: 3.4487 - val_accuracy: 0.6708 - 61s/epoch - 39ms/step\n",
            "Epoch 94/300\n",
            "1563/1563 - 62s - loss: 0.0884 - accuracy: 0.9712 - val_loss: 3.4887 - val_accuracy: 0.6728 - 62s/epoch - 39ms/step\n",
            "Epoch 95/300\n",
            "1563/1563 - 62s - loss: 0.0886 - accuracy: 0.9724 - val_loss: 3.5245 - val_accuracy: 0.6726 - 62s/epoch - 40ms/step\n",
            "Epoch 96/300\n",
            "1563/1563 - 64s - loss: 0.0861 - accuracy: 0.9726 - val_loss: 3.5033 - val_accuracy: 0.6705 - 64s/epoch - 41ms/step\n",
            "Epoch 97/300\n",
            "1563/1563 - 62s - loss: 0.0916 - accuracy: 0.9711 - val_loss: 3.4831 - val_accuracy: 0.6729 - 62s/epoch - 40ms/step\n",
            "Epoch 98/300\n",
            "1563/1563 - 62s - loss: 0.0811 - accuracy: 0.9743 - val_loss: 3.5638 - val_accuracy: 0.6745 - 62s/epoch - 40ms/step\n",
            "Epoch 99/300\n",
            "1563/1563 - 61s - loss: 0.0905 - accuracy: 0.9711 - val_loss: 3.6435 - val_accuracy: 0.6736 - 61s/epoch - 39ms/step\n",
            "Epoch 100/300\n",
            "1563/1563 - 64s - loss: 0.0764 - accuracy: 0.9743 - val_loss: 3.5962 - val_accuracy: 0.6762 - 64s/epoch - 41ms/step\n",
            "Epoch 101/300\n",
            "1563/1563 - 62s - loss: 0.0895 - accuracy: 0.9720 - val_loss: 3.5906 - val_accuracy: 0.6784 - 62s/epoch - 39ms/step\n",
            "Epoch 102/300\n",
            "1563/1563 - 62s - loss: 0.0788 - accuracy: 0.9742 - val_loss: 3.7034 - val_accuracy: 0.6793 - 62s/epoch - 40ms/step\n",
            "Epoch 103/300\n",
            "1563/1563 - 63s - loss: 0.0816 - accuracy: 0.9745 - val_loss: 3.7495 - val_accuracy: 0.6779 - 63s/epoch - 40ms/step\n",
            "Epoch 104/300\n",
            "1563/1563 - 61s - loss: 0.0855 - accuracy: 0.9731 - val_loss: 3.7552 - val_accuracy: 0.6823 - 61s/epoch - 39ms/step\n",
            "Epoch 105/300\n",
            "1563/1563 - 64s - loss: 0.0816 - accuracy: 0.9745 - val_loss: 3.8304 - val_accuracy: 0.6665 - 64s/epoch - 41ms/step\n",
            "Epoch 106/300\n",
            "1563/1563 - 64s - loss: 0.0787 - accuracy: 0.9745 - val_loss: 3.7453 - val_accuracy: 0.6738 - 64s/epoch - 41ms/step\n",
            "Epoch 107/300\n",
            "1563/1563 - 62s - loss: 0.0876 - accuracy: 0.9717 - val_loss: 3.6882 - val_accuracy: 0.6781 - 62s/epoch - 40ms/step\n",
            "Epoch 108/300\n",
            "1563/1563 - 62s - loss: 0.0860 - accuracy: 0.9730 - val_loss: 3.6065 - val_accuracy: 0.6726 - 62s/epoch - 40ms/step\n",
            "Epoch 109/300\n",
            "1563/1563 - 61s - loss: 0.0826 - accuracy: 0.9735 - val_loss: 3.7393 - val_accuracy: 0.6717 - 61s/epoch - 39ms/step\n",
            "Epoch 110/300\n",
            "1563/1563 - 64s - loss: 0.0728 - accuracy: 0.9762 - val_loss: 3.9567 - val_accuracy: 0.6721 - 64s/epoch - 41ms/step\n",
            "Epoch 111/300\n",
            "1563/1563 - 63s - loss: 0.0908 - accuracy: 0.9732 - val_loss: 3.7463 - val_accuracy: 0.6713 - 63s/epoch - 41ms/step\n",
            "Epoch 112/300\n",
            "1563/1563 - 62s - loss: 0.0812 - accuracy: 0.9752 - val_loss: 3.8206 - val_accuracy: 0.6815 - 62s/epoch - 40ms/step\n",
            "Epoch 113/300\n",
            "1563/1563 - 62s - loss: 0.0791 - accuracy: 0.9746 - val_loss: 3.8802 - val_accuracy: 0.6768 - 62s/epoch - 40ms/step\n",
            "Epoch 114/300\n",
            "1563/1563 - 61s - loss: 0.0848 - accuracy: 0.9747 - val_loss: 4.0267 - val_accuracy: 0.6684 - 61s/epoch - 39ms/step\n",
            "Epoch 115/300\n",
            "1563/1563 - 62s - loss: 0.0801 - accuracy: 0.9758 - val_loss: 3.8669 - val_accuracy: 0.6669 - 62s/epoch - 40ms/step\n",
            "Epoch 116/300\n",
            "1563/1563 - 62s - loss: 0.0817 - accuracy: 0.9736 - val_loss: 3.9310 - val_accuracy: 0.6753 - 62s/epoch - 40ms/step\n",
            "Epoch 117/300\n",
            "1563/1563 - 62s - loss: 0.0804 - accuracy: 0.9750 - val_loss: 3.9327 - val_accuracy: 0.6759 - 62s/epoch - 40ms/step\n",
            "Epoch 118/300\n",
            "1563/1563 - 64s - loss: 0.0846 - accuracy: 0.9750 - val_loss: 3.9160 - val_accuracy: 0.6758 - 64s/epoch - 41ms/step\n",
            "Epoch 119/300\n",
            "1563/1563 - 63s - loss: 0.0781 - accuracy: 0.9764 - val_loss: 3.8849 - val_accuracy: 0.6752 - 63s/epoch - 40ms/step\n",
            "Epoch 120/300\n",
            "1563/1563 - 62s - loss: 0.0809 - accuracy: 0.9757 - val_loss: 3.8350 - val_accuracy: 0.6806 - 62s/epoch - 39ms/step\n",
            "Epoch 121/300\n",
            "1563/1563 - 63s - loss: 0.0794 - accuracy: 0.9744 - val_loss: 3.8543 - val_accuracy: 0.6754 - 63s/epoch - 40ms/step\n",
            "Epoch 122/300\n",
            "1563/1563 - 62s - loss: 0.0752 - accuracy: 0.9764 - val_loss: 3.9559 - val_accuracy: 0.6756 - 62s/epoch - 39ms/step\n",
            "Epoch 123/300\n",
            "1563/1563 - 61s - loss: 0.0795 - accuracy: 0.9763 - val_loss: 4.0098 - val_accuracy: 0.6803 - 61s/epoch - 39ms/step\n",
            "Epoch 124/300\n",
            "1563/1563 - 62s - loss: 0.0783 - accuracy: 0.9751 - val_loss: 3.9460 - val_accuracy: 0.6691 - 62s/epoch - 40ms/step\n",
            "Epoch 125/300\n",
            "1563/1563 - 62s - loss: 0.0811 - accuracy: 0.9750 - val_loss: 4.0072 - val_accuracy: 0.6725 - 62s/epoch - 40ms/step\n",
            "Epoch 126/300\n",
            "1563/1563 - 64s - loss: 0.0787 - accuracy: 0.9763 - val_loss: 3.9365 - val_accuracy: 0.6803 - 64s/epoch - 41ms/step\n",
            "Epoch 127/300\n",
            "1563/1563 - 62s - loss: 0.0764 - accuracy: 0.9766 - val_loss: 4.0632 - val_accuracy: 0.6704 - 62s/epoch - 40ms/step\n",
            "Epoch 128/300\n",
            "1563/1563 - 61s - loss: 0.0661 - accuracy: 0.9791 - val_loss: 4.0980 - val_accuracy: 0.6682 - 61s/epoch - 39ms/step\n",
            "Epoch 129/300\n",
            "1563/1563 - 62s - loss: 0.0832 - accuracy: 0.9744 - val_loss: 4.3388 - val_accuracy: 0.6674 - 62s/epoch - 40ms/step\n",
            "Epoch 130/300\n",
            "1563/1563 - 62s - loss: 0.0830 - accuracy: 0.9746 - val_loss: 4.1974 - val_accuracy: 0.6699 - 62s/epoch - 40ms/step\n",
            "Epoch 131/300\n",
            "1563/1563 - 62s - loss: 0.0763 - accuracy: 0.9768 - val_loss: 4.1710 - val_accuracy: 0.6745 - 62s/epoch - 40ms/step\n",
            "Epoch 132/300\n",
            "1563/1563 - 63s - loss: 0.0773 - accuracy: 0.9767 - val_loss: 4.1889 - val_accuracy: 0.6834 - 63s/epoch - 40ms/step\n",
            "Epoch 133/300\n",
            "1563/1563 - 63s - loss: 0.0758 - accuracy: 0.9771 - val_loss: 4.0770 - val_accuracy: 0.6769 - 63s/epoch - 40ms/step\n",
            "Epoch 134/300\n",
            "1563/1563 - 64s - loss: 0.0797 - accuracy: 0.9762 - val_loss: 4.1977 - val_accuracy: 0.6795 - 64s/epoch - 41ms/step\n",
            "Epoch 135/300\n",
            "1563/1563 - 62s - loss: 0.0730 - accuracy: 0.9776 - val_loss: 4.2508 - val_accuracy: 0.6763 - 62s/epoch - 40ms/step\n",
            "Epoch 136/300\n",
            "1563/1563 - 62s - loss: 0.0807 - accuracy: 0.9770 - val_loss: 4.2021 - val_accuracy: 0.6747 - 62s/epoch - 40ms/step\n",
            "Epoch 137/300\n",
            "1563/1563 - 62s - loss: 0.0781 - accuracy: 0.9764 - val_loss: 4.1992 - val_accuracy: 0.6750 - 62s/epoch - 40ms/step\n",
            "Epoch 138/300\n",
            "1563/1563 - 61s - loss: 0.0750 - accuracy: 0.9771 - val_loss: 4.3658 - val_accuracy: 0.6739 - 61s/epoch - 39ms/step\n",
            "Epoch 139/300\n",
            "1563/1563 - 62s - loss: 0.0711 - accuracy: 0.9788 - val_loss: 4.2797 - val_accuracy: 0.6759 - 62s/epoch - 40ms/step\n",
            "Epoch 140/300\n",
            "1563/1563 - 62s - loss: 0.0822 - accuracy: 0.9759 - val_loss: 4.1657 - val_accuracy: 0.6803 - 62s/epoch - 40ms/step\n",
            "Epoch 141/300\n",
            "1563/1563 - 63s - loss: 0.0791 - accuracy: 0.9766 - val_loss: 4.3895 - val_accuracy: 0.6729 - 63s/epoch - 40ms/step\n",
            "Epoch 142/300\n",
            "1563/1563 - 63s - loss: 0.0702 - accuracy: 0.9777 - val_loss: 4.3725 - val_accuracy: 0.6760 - 63s/epoch - 40ms/step\n",
            "Epoch 143/300\n",
            "1563/1563 - 61s - loss: 0.0860 - accuracy: 0.9754 - val_loss: 4.2720 - val_accuracy: 0.6715 - 61s/epoch - 39ms/step\n",
            "Epoch 144/300\n",
            "1563/1563 - 62s - loss: 0.0731 - accuracy: 0.9782 - val_loss: 4.4726 - val_accuracy: 0.6744 - 62s/epoch - 40ms/step\n",
            "Epoch 145/300\n",
            "1563/1563 - 62s - loss: 0.0738 - accuracy: 0.9787 - val_loss: 4.5982 - val_accuracy: 0.6721 - 62s/epoch - 40ms/step\n",
            "Epoch 146/300\n",
            "1563/1563 - 62s - loss: 0.0909 - accuracy: 0.9748 - val_loss: 4.5954 - val_accuracy: 0.6761 - 62s/epoch - 40ms/step\n",
            "Epoch 147/300\n",
            "1563/1563 - 61s - loss: 0.0705 - accuracy: 0.9782 - val_loss: 4.6033 - val_accuracy: 0.6679 - 61s/epoch - 39ms/step\n",
            "Epoch 148/300\n",
            "1563/1563 - 64s - loss: 0.0775 - accuracy: 0.9775 - val_loss: 4.4171 - val_accuracy: 0.6717 - 64s/epoch - 41ms/step\n",
            "Epoch 149/300\n",
            "1563/1563 - 64s - loss: 0.0782 - accuracy: 0.9779 - val_loss: 4.6621 - val_accuracy: 0.6741 - 64s/epoch - 41ms/step\n",
            "Epoch 150/300\n",
            "1563/1563 - 62s - loss: 0.0767 - accuracy: 0.9775 - val_loss: 4.6437 - val_accuracy: 0.6724 - 62s/epoch - 40ms/step\n",
            "Epoch 151/300\n",
            "1563/1563 - 62s - loss: 0.0739 - accuracy: 0.9780 - val_loss: 4.6046 - val_accuracy: 0.6786 - 62s/epoch - 40ms/step\n",
            "Epoch 152/300\n",
            "1563/1563 - 62s - loss: 0.0699 - accuracy: 0.9799 - val_loss: 4.6926 - val_accuracy: 0.6769 - 62s/epoch - 40ms/step\n",
            "Epoch 153/300\n",
            "1563/1563 - 61s - loss: 0.0799 - accuracy: 0.9766 - val_loss: 4.5370 - val_accuracy: 0.6658 - 61s/epoch - 39ms/step\n",
            "Epoch 154/300\n",
            "1563/1563 - 65s - loss: 0.0810 - accuracy: 0.9771 - val_loss: 4.7407 - val_accuracy: 0.6747 - 65s/epoch - 41ms/step\n",
            "Epoch 155/300\n",
            "1563/1563 - 62s - loss: 0.0727 - accuracy: 0.9791 - val_loss: 4.7139 - val_accuracy: 0.6751 - 62s/epoch - 40ms/step\n",
            "Epoch 156/300\n",
            "1563/1563 - 64s - loss: 0.0747 - accuracy: 0.9786 - val_loss: 4.6957 - val_accuracy: 0.6781 - 64s/epoch - 41ms/step\n",
            "Epoch 157/300\n",
            "1563/1563 - 62s - loss: 0.0852 - accuracy: 0.9762 - val_loss: 4.7187 - val_accuracy: 0.6688 - 62s/epoch - 40ms/step\n",
            "Epoch 158/300\n",
            "1563/1563 - 62s - loss: 0.0758 - accuracy: 0.9790 - val_loss: 4.6113 - val_accuracy: 0.6759 - 62s/epoch - 40ms/step\n",
            "Epoch 159/300\n",
            "1563/1563 - 63s - loss: 0.0801 - accuracy: 0.9778 - val_loss: 4.6636 - val_accuracy: 0.6692 - 63s/epoch - 40ms/step\n",
            "Epoch 160/300\n",
            "1563/1563 - 61s - loss: 0.0677 - accuracy: 0.9801 - val_loss: 4.8269 - val_accuracy: 0.6710 - 61s/epoch - 39ms/step\n",
            "Epoch 161/300\n",
            "1563/1563 - 62s - loss: 0.0750 - accuracy: 0.9783 - val_loss: 4.6483 - val_accuracy: 0.6745 - 62s/epoch - 40ms/step\n",
            "Epoch 162/300\n",
            "1563/1563 - 64s - loss: 0.0693 - accuracy: 0.9796 - val_loss: 4.8842 - val_accuracy: 0.6693 - 64s/epoch - 41ms/step\n",
            "Epoch 163/300\n",
            "1563/1563 - 62s - loss: 0.0832 - accuracy: 0.9778 - val_loss: 4.8371 - val_accuracy: 0.6728 - 62s/epoch - 40ms/step\n",
            "Epoch 164/300\n",
            "1563/1563 - 61s - loss: 0.0865 - accuracy: 0.9766 - val_loss: 4.7983 - val_accuracy: 0.6773 - 61s/epoch - 39ms/step\n",
            "Epoch 165/300\n",
            "1563/1563 - 61s - loss: 0.0675 - accuracy: 0.9808 - val_loss: 4.8535 - val_accuracy: 0.6672 - 61s/epoch - 39ms/step\n",
            "Epoch 166/300\n",
            "1563/1563 - 62s - loss: 0.0776 - accuracy: 0.9779 - val_loss: 4.7397 - val_accuracy: 0.6710 - 62s/epoch - 40ms/step\n",
            "Epoch 167/300\n",
            "1563/1563 - 62s - loss: 0.0758 - accuracy: 0.9787 - val_loss: 5.0158 - val_accuracy: 0.6702 - 62s/epoch - 40ms/step\n",
            "Epoch 168/300\n",
            "1563/1563 - 61s - loss: 0.0785 - accuracy: 0.9784 - val_loss: 5.0206 - val_accuracy: 0.6692 - 61s/epoch - 39ms/step\n",
            "Epoch 169/300\n",
            "1563/1563 - 61s - loss: 0.0904 - accuracy: 0.9758 - val_loss: 4.7881 - val_accuracy: 0.6700 - 61s/epoch - 39ms/step\n",
            "Epoch 170/300\n",
            "1563/1563 - 62s - loss: 0.0640 - accuracy: 0.9817 - val_loss: 5.0540 - val_accuracy: 0.6739 - 62s/epoch - 40ms/step\n",
            "Epoch 171/300\n",
            "1563/1563 - 63s - loss: 0.0771 - accuracy: 0.9794 - val_loss: 4.8675 - val_accuracy: 0.6753 - 63s/epoch - 41ms/step\n",
            "Epoch 172/300\n",
            "1563/1563 - 62s - loss: 0.0840 - accuracy: 0.9771 - val_loss: 4.7893 - val_accuracy: 0.6730 - 62s/epoch - 40ms/step\n",
            "Epoch 173/300\n",
            "1563/1563 - 63s - loss: 0.0795 - accuracy: 0.9777 - val_loss: 5.0586 - val_accuracy: 0.6683 - 63s/epoch - 40ms/step\n",
            "Epoch 174/300\n",
            "1563/1563 - 63s - loss: 0.0747 - accuracy: 0.9796 - val_loss: 4.9821 - val_accuracy: 0.6732 - 63s/epoch - 40ms/step\n",
            "Epoch 175/300\n",
            "1563/1563 - 61s - loss: 0.0731 - accuracy: 0.9799 - val_loss: 4.8072 - val_accuracy: 0.6741 - 61s/epoch - 39ms/step\n",
            "Epoch 176/300\n",
            "1563/1563 - 62s - loss: 0.0712 - accuracy: 0.9794 - val_loss: 5.1062 - val_accuracy: 0.6707 - 62s/epoch - 40ms/step\n",
            "Epoch 177/300\n",
            "1563/1563 - 62s - loss: 0.0752 - accuracy: 0.9795 - val_loss: 5.1449 - val_accuracy: 0.6676 - 62s/epoch - 40ms/step\n",
            "Epoch 178/300\n",
            "1563/1563 - 63s - loss: 0.0742 - accuracy: 0.9794 - val_loss: 5.1473 - val_accuracy: 0.6699 - 63s/epoch - 40ms/step\n",
            "Epoch 179/300\n",
            "1563/1563 - 61s - loss: 0.0759 - accuracy: 0.9790 - val_loss: 4.9830 - val_accuracy: 0.6719 - 61s/epoch - 39ms/step\n",
            "Epoch 180/300\n",
            "1563/1563 - 62s - loss: 0.0849 - accuracy: 0.9778 - val_loss: 4.9681 - val_accuracy: 0.6756 - 62s/epoch - 40ms/step\n",
            "Epoch 181/300\n",
            "1563/1563 - 64s - loss: 0.0701 - accuracy: 0.9812 - val_loss: 5.0619 - val_accuracy: 0.6676 - 64s/epoch - 41ms/step\n",
            "Epoch 182/300\n",
            "1563/1563 - 62s - loss: 0.0704 - accuracy: 0.9801 - val_loss: 5.3325 - val_accuracy: 0.6788 - 62s/epoch - 40ms/step\n",
            "Epoch 183/300\n",
            "1563/1563 - 62s - loss: 0.0842 - accuracy: 0.9782 - val_loss: 5.1772 - val_accuracy: 0.6776 - 62s/epoch - 40ms/step\n",
            "Epoch 184/300\n",
            "1563/1563 - 63s - loss: 0.0786 - accuracy: 0.9790 - val_loss: 5.1131 - val_accuracy: 0.6789 - 63s/epoch - 40ms/step\n",
            "Epoch 185/300\n",
            "1563/1563 - 64s - loss: 0.0657 - accuracy: 0.9827 - val_loss: 5.5045 - val_accuracy: 0.6609 - 64s/epoch - 41ms/step\n",
            "Epoch 186/300\n",
            "1563/1563 - 63s - loss: 0.0961 - accuracy: 0.9762 - val_loss: 5.3434 - val_accuracy: 0.6736 - 63s/epoch - 40ms/step\n",
            "Epoch 187/300\n",
            "1563/1563 - 62s - loss: 0.0741 - accuracy: 0.9804 - val_loss: 5.2917 - val_accuracy: 0.6788 - 62s/epoch - 40ms/step\n",
            "Epoch 188/300\n",
            "1563/1563 - 62s - loss: 0.0679 - accuracy: 0.9817 - val_loss: 5.6238 - val_accuracy: 0.6677 - 62s/epoch - 40ms/step\n",
            "Epoch 189/300\n",
            "1563/1563 - 63s - loss: 0.0915 - accuracy: 0.9772 - val_loss: 5.6775 - val_accuracy: 0.6636 - 63s/epoch - 40ms/step\n",
            "Epoch 190/300\n",
            "1563/1563 - 63s - loss: 0.0744 - accuracy: 0.9803 - val_loss: 5.4787 - val_accuracy: 0.6781 - 63s/epoch - 40ms/step\n",
            "Epoch 191/300\n",
            "1563/1563 - 63s - loss: 0.0756 - accuracy: 0.9802 - val_loss: 5.4011 - val_accuracy: 0.6689 - 63s/epoch - 40ms/step\n",
            "Epoch 192/300\n",
            "1563/1563 - 61s - loss: 0.0689 - accuracy: 0.9819 - val_loss: 5.4095 - val_accuracy: 0.6735 - 61s/epoch - 39ms/step\n",
            "Epoch 193/300\n",
            "1563/1563 - 62s - loss: 0.0827 - accuracy: 0.9786 - val_loss: 5.2798 - val_accuracy: 0.6736 - 62s/epoch - 40ms/step\n",
            "Epoch 194/300\n",
            "1563/1563 - 62s - loss: 0.0798 - accuracy: 0.9797 - val_loss: 5.5968 - val_accuracy: 0.6641 - 62s/epoch - 40ms/step\n",
            "Epoch 195/300\n",
            "1563/1563 - 63s - loss: 0.0704 - accuracy: 0.9816 - val_loss: 5.2750 - val_accuracy: 0.6714 - 63s/epoch - 40ms/step\n",
            "Epoch 196/300\n",
            "1563/1563 - 62s - loss: 0.0759 - accuracy: 0.9799 - val_loss: 5.4548 - val_accuracy: 0.6765 - 62s/epoch - 40ms/step\n",
            "Epoch 197/300\n",
            "1563/1563 - 63s - loss: 0.0814 - accuracy: 0.9791 - val_loss: 5.5192 - val_accuracy: 0.6758 - 63s/epoch - 40ms/step\n",
            "Epoch 198/300\n",
            "1563/1563 - 63s - loss: 0.0794 - accuracy: 0.9792 - val_loss: 5.5932 - val_accuracy: 0.6663 - 63s/epoch - 40ms/step\n",
            "Epoch 199/300\n",
            "1563/1563 - 62s - loss: 0.0855 - accuracy: 0.9779 - val_loss: 5.6341 - val_accuracy: 0.6720 - 62s/epoch - 40ms/step\n",
            "Epoch 200/300\n",
            "1563/1563 - 62s - loss: 0.0738 - accuracy: 0.9811 - val_loss: 5.6965 - val_accuracy: 0.6674 - 62s/epoch - 40ms/step\n",
            "Epoch 201/300\n",
            "1563/1563 - 62s - loss: 0.0729 - accuracy: 0.9808 - val_loss: 5.4987 - val_accuracy: 0.6664 - 62s/epoch - 40ms/step\n",
            "Epoch 202/300\n",
            "1563/1563 - 61s - loss: 0.0729 - accuracy: 0.9809 - val_loss: 5.3578 - val_accuracy: 0.6752 - 61s/epoch - 39ms/step\n",
            "Epoch 203/300\n",
            "1563/1563 - 61s - loss: 0.0779 - accuracy: 0.9797 - val_loss: 5.5178 - val_accuracy: 0.6734 - 61s/epoch - 39ms/step\n",
            "Epoch 204/300\n",
            "1563/1563 - 62s - loss: 0.0773 - accuracy: 0.9796 - val_loss: 5.7628 - val_accuracy: 0.6701 - 62s/epoch - 40ms/step\n",
            "Epoch 205/300\n",
            "1563/1563 - 62s - loss: 0.0812 - accuracy: 0.9798 - val_loss: 5.8402 - val_accuracy: 0.6639 - 62s/epoch - 40ms/step\n",
            "Epoch 206/300\n",
            "1563/1563 - 61s - loss: 0.0800 - accuracy: 0.9807 - val_loss: 5.7727 - val_accuracy: 0.6706 - 61s/epoch - 39ms/step\n",
            "Epoch 207/300\n",
            "1563/1563 - 61s - loss: 0.0668 - accuracy: 0.9820 - val_loss: 5.7378 - val_accuracy: 0.6740 - 61s/epoch - 39ms/step\n",
            "Epoch 208/300\n",
            "1563/1563 - 61s - loss: 0.0728 - accuracy: 0.9806 - val_loss: 5.8242 - val_accuracy: 0.6733 - 61s/epoch - 39ms/step\n",
            "Epoch 209/300\n",
            "1563/1563 - 61s - loss: 0.0837 - accuracy: 0.9798 - val_loss: 5.7866 - val_accuracy: 0.6708 - 61s/epoch - 39ms/step\n",
            "Epoch 210/300\n",
            "1563/1563 - 62s - loss: 0.0806 - accuracy: 0.9803 - val_loss: 5.8328 - val_accuracy: 0.6659 - 62s/epoch - 40ms/step\n",
            "Epoch 211/300\n",
            "1563/1563 - 61s - loss: 0.0789 - accuracy: 0.9801 - val_loss: 6.2038 - val_accuracy: 0.6740 - 61s/epoch - 39ms/step\n",
            "Epoch 212/300\n",
            "1563/1563 - 62s - loss: 0.0703 - accuracy: 0.9824 - val_loss: 6.0892 - val_accuracy: 0.6694 - 62s/epoch - 40ms/step\n",
            "Epoch 213/300\n",
            "1563/1563 - 63s - loss: 0.0729 - accuracy: 0.9815 - val_loss: 6.1602 - val_accuracy: 0.6650 - 63s/epoch - 40ms/step\n",
            "Epoch 214/300\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-329a0281c66e>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1781\u001b[0m                         ):\n\u001b[1;32m   1782\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    868\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess the CIFAR-10 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "train_labels, test_labels = to_categorical(train_labels), to_categorical(test_labels)\n",
        "\n",
        "# Build the CNN model\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))  # Adjust the output layer for 10 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "history = model.fit(train_images, train_labels, epochs=300, validation_data=(test_images, test_labels), verbose=2)\n",
        "end_time = time.time()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nTraining Time:\", round(end_time - start_time, 2), \"seconds\")\n",
        "print(\"Training Loss:\", history.history['loss'][-1])\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ]
    }
  ]
}