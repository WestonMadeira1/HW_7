{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHPFB4DuoXzNKpBLJB332x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WestonMadeira1/HW_7/blob/main/HW7_P1B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FqFA15YBkPaa",
        "outputId": "4b21b4bc-fa7e-4461-e956-c6f784a03d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 15, 15, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 6, 6, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 4, 4, 128)         73856     \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 2, 2, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 241674 (944.04 KB)\n",
            "Trainable params: 241674 (944.04 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/300\n",
            "1563/1563 - 75s - loss: 1.5017 - accuracy: 0.4493 - val_loss: 1.1911 - val_accuracy: 0.5768 - 75s/epoch - 48ms/step\n",
            "Epoch 2/300\n",
            "1563/1563 - 71s - loss: 1.1077 - accuracy: 0.6078 - val_loss: 1.0806 - val_accuracy: 0.6199 - 71s/epoch - 46ms/step\n",
            "Epoch 3/300\n",
            "1563/1563 - 71s - loss: 0.9401 - accuracy: 0.6692 - val_loss: 0.9272 - val_accuracy: 0.6783 - 71s/epoch - 45ms/step\n",
            "Epoch 4/300\n",
            "1563/1563 - 72s - loss: 0.8369 - accuracy: 0.7062 - val_loss: 0.8659 - val_accuracy: 0.6928 - 72s/epoch - 46ms/step\n",
            "Epoch 5/300\n",
            "1563/1563 - 72s - loss: 0.7576 - accuracy: 0.7344 - val_loss: 0.8649 - val_accuracy: 0.6980 - 72s/epoch - 46ms/step\n",
            "Epoch 6/300\n",
            "1563/1563 - 72s - loss: 0.6862 - accuracy: 0.7585 - val_loss: 0.8518 - val_accuracy: 0.7136 - 72s/epoch - 46ms/step\n",
            "Epoch 7/300\n",
            "1563/1563 - 72s - loss: 0.6197 - accuracy: 0.7832 - val_loss: 0.8438 - val_accuracy: 0.7201 - 72s/epoch - 46ms/step\n",
            "Epoch 8/300\n",
            "1563/1563 - 72s - loss: 0.5694 - accuracy: 0.7982 - val_loss: 0.8880 - val_accuracy: 0.7041 - 72s/epoch - 46ms/step\n",
            "Epoch 9/300\n",
            "1563/1563 - 71s - loss: 0.5165 - accuracy: 0.8178 - val_loss: 0.8743 - val_accuracy: 0.7130 - 71s/epoch - 46ms/step\n",
            "Epoch 10/300\n",
            "1563/1563 - 72s - loss: 0.4667 - accuracy: 0.8356 - val_loss: 0.8790 - val_accuracy: 0.7230 - 72s/epoch - 46ms/step\n",
            "Epoch 11/300\n",
            "1563/1563 - 72s - loss: 0.4326 - accuracy: 0.8474 - val_loss: 0.9289 - val_accuracy: 0.7183 - 72s/epoch - 46ms/step\n",
            "Epoch 12/300\n",
            "1563/1563 - 70s - loss: 0.3870 - accuracy: 0.8603 - val_loss: 0.9714 - val_accuracy: 0.7240 - 70s/epoch - 45ms/step\n",
            "Epoch 13/300\n",
            "1563/1563 - 71s - loss: 0.3531 - accuracy: 0.8741 - val_loss: 1.0349 - val_accuracy: 0.7157 - 71s/epoch - 46ms/step\n",
            "Epoch 14/300\n",
            "1563/1563 - 72s - loss: 0.3226 - accuracy: 0.8848 - val_loss: 1.0635 - val_accuracy: 0.7225 - 72s/epoch - 46ms/step\n",
            "Epoch 15/300\n",
            "1563/1563 - 72s - loss: 0.2921 - accuracy: 0.8956 - val_loss: 1.1304 - val_accuracy: 0.7108 - 72s/epoch - 46ms/step\n",
            "Epoch 16/300\n",
            "1563/1563 - 71s - loss: 0.2710 - accuracy: 0.9047 - val_loss: 1.1296 - val_accuracy: 0.7174 - 71s/epoch - 46ms/step\n",
            "Epoch 17/300\n",
            "1563/1563 - 72s - loss: 0.2527 - accuracy: 0.9098 - val_loss: 1.1711 - val_accuracy: 0.7146 - 72s/epoch - 46ms/step\n",
            "Epoch 18/300\n",
            "1563/1563 - 71s - loss: 0.2324 - accuracy: 0.9172 - val_loss: 1.2742 - val_accuracy: 0.7148 - 71s/epoch - 45ms/step\n",
            "Epoch 19/300\n",
            "1563/1563 - 70s - loss: 0.2129 - accuracy: 0.9250 - val_loss: 1.3236 - val_accuracy: 0.7171 - 70s/epoch - 45ms/step\n",
            "Epoch 20/300\n",
            "1563/1563 - 72s - loss: 0.2079 - accuracy: 0.9259 - val_loss: 1.4111 - val_accuracy: 0.7130 - 72s/epoch - 46ms/step\n",
            "Epoch 21/300\n",
            "1563/1563 - 72s - loss: 0.1907 - accuracy: 0.9315 - val_loss: 1.4299 - val_accuracy: 0.7158 - 72s/epoch - 46ms/step\n",
            "Epoch 22/300\n",
            "1563/1563 - 74s - loss: 0.1899 - accuracy: 0.9325 - val_loss: 1.5231 - val_accuracy: 0.7092 - 74s/epoch - 47ms/step\n",
            "Epoch 23/300\n",
            "1563/1563 - 72s - loss: 0.1746 - accuracy: 0.9390 - val_loss: 1.5349 - val_accuracy: 0.6973 - 72s/epoch - 46ms/step\n",
            "Epoch 24/300\n",
            "1563/1563 - 73s - loss: 0.1621 - accuracy: 0.9426 - val_loss: 1.6133 - val_accuracy: 0.7086 - 73s/epoch - 47ms/step\n",
            "Epoch 25/300\n",
            "1563/1563 - 72s - loss: 0.1692 - accuracy: 0.9408 - val_loss: 1.6119 - val_accuracy: 0.7059 - 72s/epoch - 46ms/step\n",
            "Epoch 26/300\n",
            "1563/1563 - 73s - loss: 0.1547 - accuracy: 0.9453 - val_loss: 1.6196 - val_accuracy: 0.7087 - 73s/epoch - 47ms/step\n",
            "Epoch 27/300\n",
            "1563/1563 - 72s - loss: 0.1471 - accuracy: 0.9493 - val_loss: 1.6817 - val_accuracy: 0.7082 - 72s/epoch - 46ms/step\n",
            "Epoch 28/300\n",
            "1563/1563 - 72s - loss: 0.1420 - accuracy: 0.9521 - val_loss: 1.7581 - val_accuracy: 0.6948 - 72s/epoch - 46ms/step\n",
            "Epoch 29/300\n",
            "1563/1563 - 71s - loss: 0.1417 - accuracy: 0.9519 - val_loss: 1.7711 - val_accuracy: 0.7050 - 71s/epoch - 46ms/step\n",
            "Epoch 30/300\n",
            "1563/1563 - 70s - loss: 0.1320 - accuracy: 0.9550 - val_loss: 1.8086 - val_accuracy: 0.7105 - 70s/epoch - 45ms/step\n",
            "Epoch 31/300\n",
            "1563/1563 - 72s - loss: 0.1340 - accuracy: 0.9547 - val_loss: 1.8594 - val_accuracy: 0.6997 - 72s/epoch - 46ms/step\n",
            "Epoch 32/300\n",
            "1563/1563 - 71s - loss: 0.1268 - accuracy: 0.9577 - val_loss: 1.9841 - val_accuracy: 0.6990 - 71s/epoch - 46ms/step\n",
            "Epoch 33/300\n",
            "1563/1563 - 71s - loss: 0.1283 - accuracy: 0.9572 - val_loss: 1.9106 - val_accuracy: 0.7023 - 71s/epoch - 46ms/step\n",
            "Epoch 34/300\n",
            "1563/1563 - 76s - loss: 0.1159 - accuracy: 0.9603 - val_loss: 1.9209 - val_accuracy: 0.7018 - 76s/epoch - 49ms/step\n",
            "Epoch 35/300\n",
            "1563/1563 - 71s - loss: 0.1132 - accuracy: 0.9617 - val_loss: 2.0294 - val_accuracy: 0.7093 - 71s/epoch - 45ms/step\n",
            "Epoch 36/300\n",
            "1563/1563 - 71s - loss: 0.1209 - accuracy: 0.9586 - val_loss: 1.9007 - val_accuracy: 0.7027 - 71s/epoch - 46ms/step\n",
            "Epoch 37/300\n",
            "1563/1563 - 71s - loss: 0.1183 - accuracy: 0.9601 - val_loss: 1.9983 - val_accuracy: 0.6860 - 71s/epoch - 45ms/step\n",
            "Epoch 38/300\n",
            "1563/1563 - 70s - loss: 0.1069 - accuracy: 0.9644 - val_loss: 2.0364 - val_accuracy: 0.6994 - 70s/epoch - 45ms/step\n",
            "Epoch 39/300\n",
            "1563/1563 - 71s - loss: 0.1060 - accuracy: 0.9631 - val_loss: 2.1074 - val_accuracy: 0.7014 - 71s/epoch - 46ms/step\n",
            "Epoch 40/300\n",
            "1563/1563 - 72s - loss: 0.1096 - accuracy: 0.9637 - val_loss: 1.9960 - val_accuracy: 0.7115 - 72s/epoch - 46ms/step\n",
            "Epoch 41/300\n",
            "1563/1563 - 72s - loss: 0.1027 - accuracy: 0.9653 - val_loss: 2.0045 - val_accuracy: 0.7044 - 72s/epoch - 46ms/step\n",
            "Epoch 42/300\n",
            "1563/1563 - 71s - loss: 0.1011 - accuracy: 0.9665 - val_loss: 2.0164 - val_accuracy: 0.7014 - 71s/epoch - 46ms/step\n",
            "Epoch 43/300\n",
            "1563/1563 - 71s - loss: 0.1042 - accuracy: 0.9652 - val_loss: 2.0889 - val_accuracy: 0.7048 - 71s/epoch - 45ms/step\n",
            "Epoch 44/300\n",
            "1563/1563 - 71s - loss: 0.1002 - accuracy: 0.9666 - val_loss: 2.0696 - val_accuracy: 0.7049 - 71s/epoch - 45ms/step\n",
            "Epoch 45/300\n",
            "1563/1563 - 70s - loss: 0.1022 - accuracy: 0.9662 - val_loss: 2.1891 - val_accuracy: 0.7023 - 70s/epoch - 45ms/step\n",
            "Epoch 46/300\n",
            "1563/1563 - 73s - loss: 0.0944 - accuracy: 0.9689 - val_loss: 2.1342 - val_accuracy: 0.7079 - 73s/epoch - 47ms/step\n",
            "Epoch 47/300\n",
            "1563/1563 - 73s - loss: 0.0912 - accuracy: 0.9700 - val_loss: 2.1416 - val_accuracy: 0.7002 - 73s/epoch - 47ms/step\n",
            "Epoch 48/300\n",
            "1563/1563 - 71s - loss: 0.0909 - accuracy: 0.9694 - val_loss: 2.2611 - val_accuracy: 0.7066 - 71s/epoch - 46ms/step\n",
            "Epoch 49/300\n",
            "1563/1563 - 71s - loss: 0.0961 - accuracy: 0.9680 - val_loss: 2.3219 - val_accuracy: 0.6973 - 71s/epoch - 46ms/step\n",
            "Epoch 50/300\n",
            "1563/1563 - 71s - loss: 0.0923 - accuracy: 0.9700 - val_loss: 2.1893 - val_accuracy: 0.7142 - 71s/epoch - 46ms/step\n",
            "Epoch 51/300\n",
            "1563/1563 - 76s - loss: 0.0869 - accuracy: 0.9722 - val_loss: 2.2032 - val_accuracy: 0.7068 - 76s/epoch - 49ms/step\n",
            "Epoch 52/300\n",
            "1563/1563 - 71s - loss: 0.0847 - accuracy: 0.9723 - val_loss: 2.3296 - val_accuracy: 0.6938 - 71s/epoch - 46ms/step\n",
            "Epoch 53/300\n",
            "1563/1563 - 71s - loss: 0.0838 - accuracy: 0.9724 - val_loss: 2.2434 - val_accuracy: 0.7049 - 71s/epoch - 46ms/step\n",
            "Epoch 54/300\n",
            "1563/1563 - 70s - loss: 0.0912 - accuracy: 0.9703 - val_loss: 2.2492 - val_accuracy: 0.7000 - 70s/epoch - 45ms/step\n",
            "Epoch 55/300\n",
            "1563/1563 - 72s - loss: 0.0762 - accuracy: 0.9754 - val_loss: 2.3824 - val_accuracy: 0.7072 - 72s/epoch - 46ms/step\n",
            "Epoch 56/300\n",
            "1563/1563 - 71s - loss: 0.0851 - accuracy: 0.9723 - val_loss: 2.3198 - val_accuracy: 0.6948 - 71s/epoch - 46ms/step\n",
            "Epoch 57/300\n",
            "1563/1563 - 71s - loss: 0.0867 - accuracy: 0.9717 - val_loss: 2.2652 - val_accuracy: 0.7100 - 71s/epoch - 46ms/step\n",
            "Epoch 58/300\n",
            "1563/1563 - 71s - loss: 0.0846 - accuracy: 0.9725 - val_loss: 2.3722 - val_accuracy: 0.7016 - 71s/epoch - 46ms/step\n",
            "Epoch 59/300\n",
            "1563/1563 - 71s - loss: 0.0748 - accuracy: 0.9749 - val_loss: 2.4085 - val_accuracy: 0.7026 - 71s/epoch - 46ms/step\n",
            "Epoch 60/300\n",
            "1563/1563 - 76s - loss: 0.0800 - accuracy: 0.9739 - val_loss: 2.4367 - val_accuracy: 0.7124 - 76s/epoch - 49ms/step\n",
            "Epoch 61/300\n",
            "1563/1563 - 71s - loss: 0.0740 - accuracy: 0.9757 - val_loss: 2.3789 - val_accuracy: 0.7059 - 71s/epoch - 46ms/step\n",
            "Epoch 62/300\n",
            "1563/1563 - 72s - loss: 0.0759 - accuracy: 0.9753 - val_loss: 2.6101 - val_accuracy: 0.6988 - 72s/epoch - 46ms/step\n",
            "Epoch 63/300\n",
            "1563/1563 - 71s - loss: 0.0833 - accuracy: 0.9732 - val_loss: 2.5202 - val_accuracy: 0.7040 - 71s/epoch - 45ms/step\n",
            "Epoch 64/300\n",
            "1563/1563 - 70s - loss: 0.0775 - accuracy: 0.9754 - val_loss: 2.3806 - val_accuracy: 0.7069 - 70s/epoch - 45ms/step\n",
            "Epoch 65/300\n",
            "1563/1563 - 71s - loss: 0.0753 - accuracy: 0.9766 - val_loss: 2.4602 - val_accuracy: 0.7042 - 71s/epoch - 46ms/step\n",
            "Epoch 66/300\n",
            "1563/1563 - 74s - loss: 0.0745 - accuracy: 0.9766 - val_loss: 2.4096 - val_accuracy: 0.7061 - 74s/epoch - 47ms/step\n",
            "Epoch 67/300\n",
            "1563/1563 - 71s - loss: 0.0734 - accuracy: 0.9763 - val_loss: 2.6408 - val_accuracy: 0.7052 - 71s/epoch - 46ms/step\n",
            "Epoch 68/300\n",
            "1563/1563 - 71s - loss: 0.0722 - accuracy: 0.9773 - val_loss: 2.5821 - val_accuracy: 0.7060 - 71s/epoch - 46ms/step\n",
            "Epoch 69/300\n",
            "1563/1563 - 71s - loss: 0.0799 - accuracy: 0.9749 - val_loss: 2.4732 - val_accuracy: 0.7028 - 71s/epoch - 46ms/step\n",
            "Epoch 70/300\n",
            "1563/1563 - 71s - loss: 0.0752 - accuracy: 0.9766 - val_loss: 2.5541 - val_accuracy: 0.6995 - 71s/epoch - 45ms/step\n",
            "Epoch 71/300\n",
            "1563/1563 - 70s - loss: 0.0708 - accuracy: 0.9785 - val_loss: 2.6250 - val_accuracy: 0.7026 - 70s/epoch - 45ms/step\n",
            "Epoch 72/300\n",
            "1563/1563 - 73s - loss: 0.0765 - accuracy: 0.9759 - val_loss: 2.5322 - val_accuracy: 0.7004 - 73s/epoch - 47ms/step\n",
            "Epoch 73/300\n",
            "1563/1563 - 71s - loss: 0.0681 - accuracy: 0.9788 - val_loss: 2.5103 - val_accuracy: 0.7058 - 71s/epoch - 46ms/step\n",
            "Epoch 74/300\n",
            "1563/1563 - 72s - loss: 0.0771 - accuracy: 0.9755 - val_loss: 2.4946 - val_accuracy: 0.6958 - 72s/epoch - 46ms/step\n",
            "Epoch 75/300\n",
            "1563/1563 - 72s - loss: 0.0677 - accuracy: 0.9788 - val_loss: 2.6348 - val_accuracy: 0.6891 - 72s/epoch - 46ms/step\n",
            "Epoch 76/300\n",
            "1563/1563 - 72s - loss: 0.0717 - accuracy: 0.9769 - val_loss: 2.6036 - val_accuracy: 0.7044 - 72s/epoch - 46ms/step\n",
            "Epoch 77/300\n",
            "1563/1563 - 71s - loss: 0.0691 - accuracy: 0.9780 - val_loss: 2.7818 - val_accuracy: 0.7048 - 71s/epoch - 46ms/step\n",
            "Epoch 78/300\n",
            "1563/1563 - 71s - loss: 0.0695 - accuracy: 0.9791 - val_loss: 2.7665 - val_accuracy: 0.6991 - 71s/epoch - 45ms/step\n",
            "Epoch 79/300\n",
            "1563/1563 - 70s - loss: 0.0654 - accuracy: 0.9794 - val_loss: 2.6884 - val_accuracy: 0.7014 - 70s/epoch - 45ms/step\n",
            "Epoch 80/300\n",
            "1563/1563 - 72s - loss: 0.0731 - accuracy: 0.9767 - val_loss: 2.7323 - val_accuracy: 0.6978 - 72s/epoch - 46ms/step\n",
            "Epoch 81/300\n",
            "1563/1563 - 72s - loss: 0.0692 - accuracy: 0.9783 - val_loss: 2.6834 - val_accuracy: 0.7042 - 72s/epoch - 46ms/step\n",
            "Epoch 82/300\n",
            "1563/1563 - 72s - loss: 0.0727 - accuracy: 0.9778 - val_loss: 2.6085 - val_accuracy: 0.7030 - 72s/epoch - 46ms/step\n",
            "Epoch 83/300\n",
            "1563/1563 - 71s - loss: 0.0689 - accuracy: 0.9781 - val_loss: 2.7022 - val_accuracy: 0.7050 - 71s/epoch - 46ms/step\n",
            "Epoch 84/300\n",
            "1563/1563 - 71s - loss: 0.0658 - accuracy: 0.9790 - val_loss: 2.6932 - val_accuracy: 0.6964 - 71s/epoch - 46ms/step\n",
            "Epoch 85/300\n",
            "1563/1563 - 71s - loss: 0.0691 - accuracy: 0.9785 - val_loss: 2.7754 - val_accuracy: 0.6938 - 71s/epoch - 45ms/step\n",
            "Epoch 86/300\n",
            "1563/1563 - 70s - loss: 0.0671 - accuracy: 0.9804 - val_loss: 2.5668 - val_accuracy: 0.7031 - 70s/epoch - 45ms/step\n",
            "Epoch 87/300\n",
            "1563/1563 - 73s - loss: 0.0631 - accuracy: 0.9804 - val_loss: 2.6297 - val_accuracy: 0.7022 - 73s/epoch - 47ms/step\n",
            "Epoch 88/300\n",
            "1563/1563 - 72s - loss: 0.0718 - accuracy: 0.9785 - val_loss: 2.7334 - val_accuracy: 0.6973 - 72s/epoch - 46ms/step\n",
            "Epoch 89/300\n",
            "1563/1563 - 71s - loss: 0.0667 - accuracy: 0.9793 - val_loss: 2.7413 - val_accuracy: 0.7033 - 71s/epoch - 46ms/step\n",
            "Epoch 90/300\n",
            "1563/1563 - 71s - loss: 0.0615 - accuracy: 0.9814 - val_loss: 2.7008 - val_accuracy: 0.6963 - 71s/epoch - 46ms/step\n",
            "Epoch 91/300\n",
            "1563/1563 - 72s - loss: 0.0651 - accuracy: 0.9797 - val_loss: 2.7425 - val_accuracy: 0.7023 - 72s/epoch - 46ms/step\n",
            "Epoch 92/300\n",
            "1563/1563 - 71s - loss: 0.0692 - accuracy: 0.9790 - val_loss: 2.6328 - val_accuracy: 0.7025 - 71s/epoch - 45ms/step\n",
            "Epoch 93/300\n",
            "1563/1563 - 71s - loss: 0.0620 - accuracy: 0.9815 - val_loss: 2.6613 - val_accuracy: 0.7023 - 71s/epoch - 46ms/step\n",
            "Epoch 94/300\n",
            "1563/1563 - 70s - loss: 0.0639 - accuracy: 0.9802 - val_loss: 2.7757 - val_accuracy: 0.7003 - 70s/epoch - 45ms/step\n",
            "Epoch 95/300\n",
            "1563/1563 - 72s - loss: 0.0670 - accuracy: 0.9802 - val_loss: 2.7622 - val_accuracy: 0.7026 - 72s/epoch - 46ms/step\n",
            "Epoch 96/300\n",
            "1563/1563 - 72s - loss: 0.0680 - accuracy: 0.9802 - val_loss: 2.8325 - val_accuracy: 0.6968 - 72s/epoch - 46ms/step\n",
            "Epoch 97/300\n",
            "1563/1563 - 72s - loss: 0.0590 - accuracy: 0.9818 - val_loss: 2.9967 - val_accuracy: 0.6983 - 72s/epoch - 46ms/step\n",
            "Epoch 98/300\n",
            "1563/1563 - 76s - loss: 0.0674 - accuracy: 0.9801 - val_loss: 2.7561 - val_accuracy: 0.7057 - 76s/epoch - 49ms/step\n",
            "Epoch 99/300\n",
            "1563/1563 - 72s - loss: 0.0644 - accuracy: 0.9813 - val_loss: 3.0006 - val_accuracy: 0.6863 - 72s/epoch - 46ms/step\n",
            "Epoch 100/300\n",
            "1563/1563 - 72s - loss: 0.0638 - accuracy: 0.9807 - val_loss: 2.9701 - val_accuracy: 0.7035 - 72s/epoch - 46ms/step\n",
            "Epoch 101/300\n",
            "1563/1563 - 72s - loss: 0.0633 - accuracy: 0.9809 - val_loss: 2.8404 - val_accuracy: 0.7030 - 72s/epoch - 46ms/step\n",
            "Epoch 102/300\n",
            "1563/1563 - 77s - loss: 0.0528 - accuracy: 0.9842 - val_loss: 2.9789 - val_accuracy: 0.7011 - 77s/epoch - 49ms/step\n",
            "Epoch 103/300\n",
            "1563/1563 - 73s - loss: 0.0742 - accuracy: 0.9783 - val_loss: 2.9288 - val_accuracy: 0.7013 - 73s/epoch - 47ms/step\n",
            "Epoch 104/300\n",
            "1563/1563 - 72s - loss: 0.0593 - accuracy: 0.9824 - val_loss: 2.8959 - val_accuracy: 0.7054 - 72s/epoch - 46ms/step\n",
            "Epoch 105/300\n",
            "1563/1563 - 72s - loss: 0.0651 - accuracy: 0.9810 - val_loss: 2.9214 - val_accuracy: 0.6944 - 72s/epoch - 46ms/step\n",
            "Epoch 106/300\n",
            "1563/1563 - 71s - loss: 0.0572 - accuracy: 0.9826 - val_loss: 3.0193 - val_accuracy: 0.6986 - 71s/epoch - 46ms/step\n",
            "Epoch 107/300\n",
            "1563/1563 - 70s - loss: 0.0635 - accuracy: 0.9808 - val_loss: 2.9713 - val_accuracy: 0.7039 - 70s/epoch - 45ms/step\n",
            "Epoch 108/300\n",
            "1563/1563 - 73s - loss: 0.0615 - accuracy: 0.9814 - val_loss: 3.1144 - val_accuracy: 0.6917 - 73s/epoch - 47ms/step\n",
            "Epoch 109/300\n",
            "1563/1563 - 71s - loss: 0.0620 - accuracy: 0.9819 - val_loss: 2.9100 - val_accuracy: 0.7048 - 71s/epoch - 46ms/step\n",
            "Epoch 110/300\n",
            "1563/1563 - 72s - loss: 0.0555 - accuracy: 0.9829 - val_loss: 3.0951 - val_accuracy: 0.6992 - 72s/epoch - 46ms/step\n",
            "Epoch 111/300\n",
            "1563/1563 - 72s - loss: 0.0572 - accuracy: 0.9828 - val_loss: 3.0255 - val_accuracy: 0.7024 - 72s/epoch - 46ms/step\n",
            "Epoch 112/300\n",
            "1563/1563 - 72s - loss: 0.0667 - accuracy: 0.9812 - val_loss: 2.9006 - val_accuracy: 0.7074 - 72s/epoch - 46ms/step\n",
            "Epoch 113/300\n",
            "1563/1563 - 72s - loss: 0.0567 - accuracy: 0.9832 - val_loss: 2.9308 - val_accuracy: 0.7044 - 72s/epoch - 46ms/step\n",
            "Epoch 114/300\n",
            "1563/1563 - 71s - loss: 0.0539 - accuracy: 0.9850 - val_loss: 3.0985 - val_accuracy: 0.6963 - 71s/epoch - 46ms/step\n",
            "Epoch 115/300\n",
            "1563/1563 - 72s - loss: 0.0589 - accuracy: 0.9831 - val_loss: 3.0221 - val_accuracy: 0.6983 - 72s/epoch - 46ms/step\n",
            "Epoch 116/300\n",
            "1563/1563 - 72s - loss: 0.0635 - accuracy: 0.9815 - val_loss: 2.9997 - val_accuracy: 0.7065 - 72s/epoch - 46ms/step\n",
            "Epoch 117/300\n",
            "1563/1563 - 71s - loss: 0.0583 - accuracy: 0.9825 - val_loss: 3.0861 - val_accuracy: 0.6956 - 71s/epoch - 45ms/step\n",
            "Epoch 118/300\n",
            "1563/1563 - 72s - loss: 0.0665 - accuracy: 0.9810 - val_loss: 3.1184 - val_accuracy: 0.7009 - 72s/epoch - 46ms/step\n",
            "Epoch 119/300\n",
            "1563/1563 - 73s - loss: 0.0652 - accuracy: 0.9814 - val_loss: 2.8891 - val_accuracy: 0.7039 - 73s/epoch - 47ms/step\n",
            "Epoch 120/300\n",
            "1563/1563 - 73s - loss: 0.0605 - accuracy: 0.9831 - val_loss: 3.0041 - val_accuracy: 0.7099 - 73s/epoch - 47ms/step\n",
            "Epoch 121/300\n",
            "1563/1563 - 74s - loss: 0.0579 - accuracy: 0.9831 - val_loss: 3.0759 - val_accuracy: 0.7005 - 74s/epoch - 47ms/step\n",
            "Epoch 122/300\n",
            "1563/1563 - 72s - loss: 0.0626 - accuracy: 0.9823 - val_loss: 3.1097 - val_accuracy: 0.7034 - 72s/epoch - 46ms/step\n",
            "Epoch 123/300\n",
            "1563/1563 - 72s - loss: 0.0559 - accuracy: 0.9835 - val_loss: 3.0105 - val_accuracy: 0.7101 - 72s/epoch - 46ms/step\n",
            "Epoch 124/300\n",
            "1563/1563 - 73s - loss: 0.0633 - accuracy: 0.9818 - val_loss: 3.2254 - val_accuracy: 0.7002 - 73s/epoch - 47ms/step\n",
            "Epoch 125/300\n",
            "1563/1563 - 72s - loss: 0.0625 - accuracy: 0.9824 - val_loss: 3.1792 - val_accuracy: 0.6997 - 72s/epoch - 46ms/step\n",
            "Epoch 126/300\n",
            "1563/1563 - 72s - loss: 0.0598 - accuracy: 0.9836 - val_loss: 3.1754 - val_accuracy: 0.7053 - 72s/epoch - 46ms/step\n",
            "Epoch 127/300\n",
            "1563/1563 - 76s - loss: 0.0563 - accuracy: 0.9837 - val_loss: 3.0723 - val_accuracy: 0.7083 - 76s/epoch - 49ms/step\n",
            "Epoch 128/300\n",
            "1563/1563 - 72s - loss: 0.0546 - accuracy: 0.9844 - val_loss: 3.0472 - val_accuracy: 0.7086 - 72s/epoch - 46ms/step\n",
            "Epoch 129/300\n",
            "1563/1563 - 72s - loss: 0.0604 - accuracy: 0.9832 - val_loss: 3.3064 - val_accuracy: 0.6947 - 72s/epoch - 46ms/step\n",
            "Epoch 130/300\n",
            "1563/1563 - 76s - loss: 0.0607 - accuracy: 0.9829 - val_loss: 3.1343 - val_accuracy: 0.7023 - 76s/epoch - 49ms/step\n",
            "Epoch 131/300\n",
            "1563/1563 - 72s - loss: 0.0606 - accuracy: 0.9831 - val_loss: 3.1957 - val_accuracy: 0.7056 - 72s/epoch - 46ms/step\n",
            "Epoch 132/300\n",
            "1563/1563 - 75s - loss: 0.0529 - accuracy: 0.9849 - val_loss: 3.4841 - val_accuracy: 0.6934 - 75s/epoch - 48ms/step\n",
            "Epoch 133/300\n",
            "1563/1563 - 72s - loss: 0.0595 - accuracy: 0.9832 - val_loss: 3.2604 - val_accuracy: 0.6971 - 72s/epoch - 46ms/step\n",
            "Epoch 134/300\n",
            "1563/1563 - 72s - loss: 0.0652 - accuracy: 0.9828 - val_loss: 3.2898 - val_accuracy: 0.6964 - 72s/epoch - 46ms/step\n",
            "Epoch 135/300\n",
            "1563/1563 - 71s - loss: 0.0547 - accuracy: 0.9853 - val_loss: 3.2108 - val_accuracy: 0.7050 - 71s/epoch - 46ms/step\n",
            "Epoch 136/300\n",
            "1563/1563 - 77s - loss: 0.0560 - accuracy: 0.9837 - val_loss: 3.4025 - val_accuracy: 0.6961 - 77s/epoch - 50ms/step\n",
            "Epoch 137/300\n",
            "1563/1563 - 77s - loss: 0.0617 - accuracy: 0.9826 - val_loss: 3.2137 - val_accuracy: 0.7014 - 77s/epoch - 49ms/step\n",
            "Epoch 138/300\n",
            "1563/1563 - 72s - loss: 0.0535 - accuracy: 0.9845 - val_loss: 3.1268 - val_accuracy: 0.7049 - 72s/epoch - 46ms/step\n",
            "Epoch 139/300\n",
            "1563/1563 - 72s - loss: 0.0519 - accuracy: 0.9849 - val_loss: 3.2830 - val_accuracy: 0.6955 - 72s/epoch - 46ms/step\n",
            "Epoch 140/300\n",
            "1563/1563 - 72s - loss: 0.0594 - accuracy: 0.9838 - val_loss: 3.1953 - val_accuracy: 0.7102 - 72s/epoch - 46ms/step\n",
            "Epoch 141/300\n",
            "1563/1563 - 77s - loss: 0.0557 - accuracy: 0.9839 - val_loss: 3.1271 - val_accuracy: 0.7051 - 77s/epoch - 49ms/step\n",
            "Epoch 142/300\n",
            "1563/1563 - 74s - loss: 0.0569 - accuracy: 0.9841 - val_loss: 3.3582 - val_accuracy: 0.6919 - 74s/epoch - 47ms/step\n",
            "Epoch 143/300\n",
            "1563/1563 - 72s - loss: 0.0646 - accuracy: 0.9832 - val_loss: 3.0506 - val_accuracy: 0.6984 - 72s/epoch - 46ms/step\n",
            "Epoch 144/300\n",
            "1563/1563 - 72s - loss: 0.0525 - accuracy: 0.9852 - val_loss: 3.0847 - val_accuracy: 0.7037 - 72s/epoch - 46ms/step\n",
            "Epoch 145/300\n",
            "1563/1563 - 72s - loss: 0.0586 - accuracy: 0.9840 - val_loss: 3.1225 - val_accuracy: 0.7071 - 72s/epoch - 46ms/step\n",
            "Epoch 146/300\n",
            "1563/1563 - 77s - loss: 0.0478 - accuracy: 0.9862 - val_loss: 3.4262 - val_accuracy: 0.7000 - 77s/epoch - 49ms/step\n",
            "Epoch 147/300\n",
            "1563/1563 - 73s - loss: 0.0626 - accuracy: 0.9836 - val_loss: 3.4674 - val_accuracy: 0.6955 - 73s/epoch - 46ms/step\n",
            "Epoch 148/300\n",
            "1563/1563 - 72s - loss: 0.0580 - accuracy: 0.9838 - val_loss: 3.1666 - val_accuracy: 0.7023 - 72s/epoch - 46ms/step\n",
            "Epoch 149/300\n",
            "1563/1563 - 72s - loss: 0.0621 - accuracy: 0.9834 - val_loss: 3.2035 - val_accuracy: 0.6959 - 72s/epoch - 46ms/step\n",
            "Epoch 150/300\n",
            "1563/1563 - 77s - loss: 0.0608 - accuracy: 0.9836 - val_loss: 3.2439 - val_accuracy: 0.6958 - 77s/epoch - 49ms/step\n",
            "Epoch 151/300\n",
            "1563/1563 - 72s - loss: 0.0564 - accuracy: 0.9851 - val_loss: 3.1732 - val_accuracy: 0.7094 - 72s/epoch - 46ms/step\n",
            "Epoch 152/300\n",
            "1563/1563 - 73s - loss: 0.0537 - accuracy: 0.9851 - val_loss: 3.3224 - val_accuracy: 0.6969 - 73s/epoch - 47ms/step\n",
            "Epoch 153/300\n",
            "1563/1563 - 73s - loss: 0.0573 - accuracy: 0.9846 - val_loss: 3.4770 - val_accuracy: 0.7006 - 73s/epoch - 47ms/step\n",
            "Epoch 154/300\n",
            "1563/1563 - 74s - loss: 0.0583 - accuracy: 0.9847 - val_loss: 3.2599 - val_accuracy: 0.7028 - 74s/epoch - 47ms/step\n",
            "Epoch 155/300\n",
            "1563/1563 - 74s - loss: 0.0571 - accuracy: 0.9848 - val_loss: 3.3107 - val_accuracy: 0.7000 - 74s/epoch - 47ms/step\n",
            "Epoch 156/300\n",
            "1563/1563 - 73s - loss: 0.0471 - accuracy: 0.9865 - val_loss: 3.5623 - val_accuracy: 0.7033 - 73s/epoch - 46ms/step\n",
            "Epoch 157/300\n",
            "1563/1563 - 72s - loss: 0.0610 - accuracy: 0.9843 - val_loss: 3.3360 - val_accuracy: 0.6991 - 72s/epoch - 46ms/step\n",
            "Epoch 158/300\n",
            "1563/1563 - 71s - loss: 0.0582 - accuracy: 0.9848 - val_loss: 3.2630 - val_accuracy: 0.7027 - 71s/epoch - 45ms/step\n",
            "Epoch 159/300\n",
            "1563/1563 - 70s - loss: 0.0500 - accuracy: 0.9859 - val_loss: 3.4537 - val_accuracy: 0.7047 - 70s/epoch - 45ms/step\n",
            "Epoch 160/300\n",
            "1563/1563 - 72s - loss: 0.0650 - accuracy: 0.9825 - val_loss: 3.3955 - val_accuracy: 0.6964 - 72s/epoch - 46ms/step\n",
            "Epoch 161/300\n",
            "1563/1563 - 73s - loss: 0.0524 - accuracy: 0.9859 - val_loss: 3.5546 - val_accuracy: 0.7052 - 73s/epoch - 47ms/step\n",
            "Epoch 162/300\n",
            "1563/1563 - 72s - loss: 0.0628 - accuracy: 0.9845 - val_loss: 3.1934 - val_accuracy: 0.7096 - 72s/epoch - 46ms/step\n",
            "Epoch 163/300\n",
            "1563/1563 - 72s - loss: 0.0541 - accuracy: 0.9862 - val_loss: 3.3487 - val_accuracy: 0.6972 - 72s/epoch - 46ms/step\n",
            "Epoch 164/300\n",
            "1563/1563 - 73s - loss: 0.0563 - accuracy: 0.9851 - val_loss: 3.3167 - val_accuracy: 0.7034 - 73s/epoch - 47ms/step\n",
            "Epoch 165/300\n",
            "1563/1563 - 72s - loss: 0.0589 - accuracy: 0.9844 - val_loss: 3.3392 - val_accuracy: 0.7040 - 72s/epoch - 46ms/step\n",
            "Epoch 166/300\n",
            "1563/1563 - 72s - loss: 0.0539 - accuracy: 0.9863 - val_loss: 3.5162 - val_accuracy: 0.6974 - 72s/epoch - 46ms/step\n",
            "Epoch 167/300\n",
            "1563/1563 - 77s - loss: 0.0605 - accuracy: 0.9850 - val_loss: 3.4037 - val_accuracy: 0.7019 - 77s/epoch - 49ms/step\n",
            "Epoch 168/300\n",
            "1563/1563 - 75s - loss: 0.0512 - accuracy: 0.9860 - val_loss: 3.4563 - val_accuracy: 0.7013 - 75s/epoch - 48ms/step\n",
            "Epoch 169/300\n",
            "1563/1563 - 72s - loss: 0.0527 - accuracy: 0.9854 - val_loss: 3.4941 - val_accuracy: 0.7020 - 72s/epoch - 46ms/step\n",
            "Epoch 170/300\n",
            "1563/1563 - 72s - loss: 0.0570 - accuracy: 0.9850 - val_loss: 3.4265 - val_accuracy: 0.7031 - 72s/epoch - 46ms/step\n",
            "Epoch 171/300\n",
            "1563/1563 - 77s - loss: 0.0538 - accuracy: 0.9854 - val_loss: 3.4013 - val_accuracy: 0.6991 - 77s/epoch - 49ms/step\n",
            "Epoch 172/300\n",
            "1563/1563 - 72s - loss: 0.0582 - accuracy: 0.9853 - val_loss: 3.4625 - val_accuracy: 0.6932 - 72s/epoch - 46ms/step\n",
            "Epoch 173/300\n",
            "1563/1563 - 73s - loss: 0.0545 - accuracy: 0.9863 - val_loss: 3.5643 - val_accuracy: 0.7014 - 73s/epoch - 47ms/step\n",
            "Epoch 174/300\n",
            "1563/1563 - 72s - loss: 0.0564 - accuracy: 0.9857 - val_loss: 3.6165 - val_accuracy: 0.7010 - 72s/epoch - 46ms/step\n",
            "Epoch 175/300\n",
            "1563/1563 - 77s - loss: 0.0494 - accuracy: 0.9865 - val_loss: 3.5255 - val_accuracy: 0.7058 - 77s/epoch - 49ms/step\n",
            "Epoch 176/300\n",
            "1563/1563 - 69s - loss: 0.0536 - accuracy: 0.9854 - val_loss: 3.8764 - val_accuracy: 0.6993 - 69s/epoch - 44ms/step\n",
            "Epoch 177/300\n",
            "1563/1563 - 69s - loss: 0.0564 - accuracy: 0.9857 - val_loss: 3.4225 - val_accuracy: 0.7065 - 69s/epoch - 44ms/step\n",
            "Epoch 178/300\n",
            "1563/1563 - 68s - loss: 0.0504 - accuracy: 0.9869 - val_loss: 3.3972 - val_accuracy: 0.7105 - 68s/epoch - 44ms/step\n",
            "Epoch 179/300\n",
            "1563/1563 - 69s - loss: 0.0502 - accuracy: 0.9870 - val_loss: 3.5011 - val_accuracy: 0.7009 - 69s/epoch - 44ms/step\n",
            "Epoch 180/300\n",
            "1563/1563 - 70s - loss: 0.0571 - accuracy: 0.9850 - val_loss: 3.3603 - val_accuracy: 0.6996 - 70s/epoch - 44ms/step\n",
            "Epoch 181/300\n",
            "1563/1563 - 69s - loss: 0.0551 - accuracy: 0.9864 - val_loss: 3.5294 - val_accuracy: 0.7020 - 69s/epoch - 44ms/step\n",
            "Epoch 182/300\n",
            "1563/1563 - 68s - loss: 0.0478 - accuracy: 0.9875 - val_loss: 3.5550 - val_accuracy: 0.6974 - 68s/epoch - 43ms/step\n",
            "Epoch 183/300\n",
            "1563/1563 - 70s - loss: 0.0586 - accuracy: 0.9850 - val_loss: 3.4926 - val_accuracy: 0.7043 - 70s/epoch - 45ms/step\n",
            "Epoch 184/300\n",
            "1563/1563 - 69s - loss: 0.0499 - accuracy: 0.9868 - val_loss: 3.6898 - val_accuracy: 0.7041 - 69s/epoch - 44ms/step\n",
            "Epoch 185/300\n",
            "1563/1563 - 70s - loss: 0.0568 - accuracy: 0.9853 - val_loss: 3.5952 - val_accuracy: 0.7010 - 70s/epoch - 45ms/step\n",
            "Epoch 186/300\n",
            "1563/1563 - 70s - loss: 0.0506 - accuracy: 0.9868 - val_loss: 3.8051 - val_accuracy: 0.6965 - 70s/epoch - 45ms/step\n",
            "Epoch 187/300\n",
            "1563/1563 - 68s - loss: 0.0590 - accuracy: 0.9852 - val_loss: 3.6946 - val_accuracy: 0.7000 - 68s/epoch - 44ms/step\n",
            "Epoch 188/300\n",
            "1563/1563 - 69s - loss: 0.0525 - accuracy: 0.9874 - val_loss: 3.7097 - val_accuracy: 0.6970 - 69s/epoch - 44ms/step\n",
            "Epoch 189/300\n",
            "1563/1563 - 69s - loss: 0.0572 - accuracy: 0.9859 - val_loss: 3.5450 - val_accuracy: 0.7055 - 69s/epoch - 44ms/step\n",
            "Epoch 190/300\n",
            "1563/1563 - 69s - loss: 0.0522 - accuracy: 0.9864 - val_loss: 3.4845 - val_accuracy: 0.6934 - 69s/epoch - 44ms/step\n",
            "Epoch 191/300\n",
            "1563/1563 - 70s - loss: 0.0583 - accuracy: 0.9856 - val_loss: 3.6027 - val_accuracy: 0.7074 - 70s/epoch - 45ms/step\n",
            "Epoch 192/300\n",
            "1563/1563 - 70s - loss: 0.0417 - accuracy: 0.9887 - val_loss: 3.9044 - val_accuracy: 0.7001 - 70s/epoch - 45ms/step\n",
            "Epoch 193/300\n",
            "1563/1563 - 68s - loss: 0.0500 - accuracy: 0.9870 - val_loss: 3.9431 - val_accuracy: 0.7011 - 68s/epoch - 44ms/step\n",
            "Epoch 194/300\n",
            "1563/1563 - 68s - loss: 0.0545 - accuracy: 0.9864 - val_loss: 3.6640 - val_accuracy: 0.7067 - 68s/epoch - 43ms/step\n",
            "Epoch 195/300\n",
            "1563/1563 - 68s - loss: 0.0521 - accuracy: 0.9863 - val_loss: 3.6632 - val_accuracy: 0.7020 - 68s/epoch - 43ms/step\n",
            "Epoch 196/300\n",
            "1563/1563 - 69s - loss: 0.0625 - accuracy: 0.9851 - val_loss: 3.5123 - val_accuracy: 0.6997 - 69s/epoch - 44ms/step\n",
            "Epoch 197/300\n",
            "1563/1563 - 68s - loss: 0.0500 - accuracy: 0.9875 - val_loss: 3.5866 - val_accuracy: 0.7073 - 68s/epoch - 43ms/step\n",
            "Epoch 198/300\n",
            "1563/1563 - 70s - loss: 0.0525 - accuracy: 0.9868 - val_loss: 3.6827 - val_accuracy: 0.7070 - 70s/epoch - 45ms/step\n",
            "Epoch 199/300\n",
            "1563/1563 - 70s - loss: 0.0505 - accuracy: 0.9876 - val_loss: 3.7292 - val_accuracy: 0.7076 - 70s/epoch - 45ms/step\n",
            "Epoch 200/300\n",
            "1563/1563 - 69s - loss: 0.0514 - accuracy: 0.9862 - val_loss: 3.7264 - val_accuracy: 0.7037 - 69s/epoch - 44ms/step\n",
            "Epoch 201/300\n",
            "1563/1563 - 68s - loss: 0.0494 - accuracy: 0.9875 - val_loss: 3.7222 - val_accuracy: 0.6993 - 68s/epoch - 44ms/step\n",
            "Epoch 202/300\n",
            "1563/1563 - 69s - loss: 0.0587 - accuracy: 0.9861 - val_loss: 3.6301 - val_accuracy: 0.7071 - 69s/epoch - 44ms/step\n",
            "Epoch 203/300\n",
            "1563/1563 - 68s - loss: 0.0559 - accuracy: 0.9858 - val_loss: 3.6663 - val_accuracy: 0.7048 - 68s/epoch - 44ms/step\n",
            "Epoch 204/300\n",
            "1563/1563 - 68s - loss: 0.0522 - accuracy: 0.9876 - val_loss: 3.6244 - val_accuracy: 0.7070 - 68s/epoch - 44ms/step\n",
            "Epoch 205/300\n",
            "1563/1563 - 68s - loss: 0.0529 - accuracy: 0.9879 - val_loss: 3.6990 - val_accuracy: 0.6987 - 68s/epoch - 44ms/step\n",
            "Epoch 206/300\n",
            "1563/1563 - 69s - loss: 0.0509 - accuracy: 0.9877 - val_loss: 3.7819 - val_accuracy: 0.6894 - 69s/epoch - 44ms/step\n",
            "Epoch 207/300\n",
            "1563/1563 - 68s - loss: 0.0548 - accuracy: 0.9868 - val_loss: 3.9160 - val_accuracy: 0.6891 - 68s/epoch - 44ms/step\n",
            "Epoch 208/300\n",
            "1563/1563 - 70s - loss: 0.0484 - accuracy: 0.9882 - val_loss: 3.5810 - val_accuracy: 0.7071 - 70s/epoch - 45ms/step\n",
            "Epoch 209/300\n",
            "1563/1563 - 69s - loss: 0.0444 - accuracy: 0.9886 - val_loss: 3.9273 - val_accuracy: 0.6995 - 69s/epoch - 44ms/step\n",
            "Epoch 210/300\n",
            "1563/1563 - 68s - loss: 0.0562 - accuracy: 0.9863 - val_loss: 3.6559 - val_accuracy: 0.7002 - 68s/epoch - 43ms/step\n",
            "Epoch 211/300\n",
            "1563/1563 - 69s - loss: 0.0536 - accuracy: 0.9871 - val_loss: 3.7917 - val_accuracy: 0.7079 - 69s/epoch - 44ms/step\n",
            "Epoch 212/300\n",
            "1563/1563 - 69s - loss: 0.0483 - accuracy: 0.9880 - val_loss: 3.7651 - val_accuracy: 0.7058 - 69s/epoch - 44ms/step\n",
            "Epoch 213/300\n",
            "1563/1563 - 68s - loss: 0.0593 - accuracy: 0.9860 - val_loss: 3.8492 - val_accuracy: 0.6980 - 68s/epoch - 43ms/step\n",
            "Epoch 214/300\n",
            "1563/1563 - 69s - loss: 0.0457 - accuracy: 0.9883 - val_loss: 3.7119 - val_accuracy: 0.7052 - 69s/epoch - 44ms/step\n",
            "Epoch 215/300\n",
            "1563/1563 - 69s - loss: 0.0520 - accuracy: 0.9875 - val_loss: 3.6624 - val_accuracy: 0.7085 - 69s/epoch - 44ms/step\n",
            "Epoch 216/300\n",
            "1563/1563 - 68s - loss: 0.0508 - accuracy: 0.9868 - val_loss: 3.6716 - val_accuracy: 0.7071 - 68s/epoch - 43ms/step\n",
            "Epoch 217/300\n",
            "1563/1563 - 68s - loss: 0.0513 - accuracy: 0.9876 - val_loss: 3.8714 - val_accuracy: 0.7093 - 68s/epoch - 43ms/step\n",
            "Epoch 218/300\n",
            "1563/1563 - 68s - loss: 0.0537 - accuracy: 0.9874 - val_loss: 3.5822 - val_accuracy: 0.6991 - 68s/epoch - 43ms/step\n",
            "Epoch 219/300\n",
            "1563/1563 - 69s - loss: 0.0464 - accuracy: 0.9884 - val_loss: 3.8579 - val_accuracy: 0.7037 - 69s/epoch - 44ms/step\n",
            "Epoch 220/300\n",
            "1563/1563 - 68s - loss: 0.0566 - accuracy: 0.9864 - val_loss: 3.6843 - val_accuracy: 0.7041 - 68s/epoch - 43ms/step\n",
            "Epoch 221/300\n",
            "1563/1563 - 69s - loss: 0.0567 - accuracy: 0.9865 - val_loss: 3.6001 - val_accuracy: 0.7036 - 69s/epoch - 44ms/step\n",
            "Epoch 222/300\n",
            "1563/1563 - 68s - loss: 0.0396 - accuracy: 0.9912 - val_loss: 3.7980 - val_accuracy: 0.7049 - 68s/epoch - 43ms/step\n",
            "Epoch 223/300\n",
            "1563/1563 - 69s - loss: 0.0507 - accuracy: 0.9880 - val_loss: 3.8447 - val_accuracy: 0.7048 - 69s/epoch - 44ms/step\n",
            "Epoch 224/300\n",
            "1563/1563 - 68s - loss: 0.0537 - accuracy: 0.9876 - val_loss: 3.9260 - val_accuracy: 0.7036 - 68s/epoch - 43ms/step\n",
            "Epoch 225/300\n",
            "1563/1563 - 68s - loss: 0.0550 - accuracy: 0.9879 - val_loss: 3.8048 - val_accuracy: 0.7117 - 68s/epoch - 43ms/step\n",
            "Epoch 226/300\n",
            "1563/1563 - 69s - loss: 0.0541 - accuracy: 0.9879 - val_loss: 3.4894 - val_accuracy: 0.7079 - 69s/epoch - 44ms/step\n",
            "Epoch 227/300\n",
            "1563/1563 - 69s - loss: 0.0434 - accuracy: 0.9891 - val_loss: 3.7608 - val_accuracy: 0.7053 - 69s/epoch - 44ms/step\n",
            "Epoch 228/300\n",
            "1563/1563 - 69s - loss: 0.0474 - accuracy: 0.9887 - val_loss: 3.9280 - val_accuracy: 0.6929 - 69s/epoch - 44ms/step\n",
            "Epoch 229/300\n",
            "1563/1563 - 67s - loss: 0.0562 - accuracy: 0.9875 - val_loss: 3.5245 - val_accuracy: 0.7026 - 67s/epoch - 43ms/step\n",
            "Epoch 230/300\n",
            "1563/1563 - 69s - loss: 0.0463 - accuracy: 0.9893 - val_loss: 3.7582 - val_accuracy: 0.7048 - 69s/epoch - 44ms/step\n",
            "Epoch 231/300\n",
            "1563/1563 - 69s - loss: 0.0534 - accuracy: 0.9878 - val_loss: 3.7422 - val_accuracy: 0.7020 - 69s/epoch - 44ms/step\n",
            "Epoch 232/300\n",
            "1563/1563 - 69s - loss: 0.0472 - accuracy: 0.9883 - val_loss: 3.7858 - val_accuracy: 0.7029 - 69s/epoch - 44ms/step\n",
            "Epoch 233/300\n",
            "1563/1563 - 69s - loss: 0.0507 - accuracy: 0.9881 - val_loss: 3.5802 - val_accuracy: 0.7103 - 69s/epoch - 44ms/step\n",
            "Epoch 234/300\n",
            "1563/1563 - 68s - loss: 0.0546 - accuracy: 0.9877 - val_loss: 3.5953 - val_accuracy: 0.7052 - 68s/epoch - 43ms/step\n",
            "Epoch 235/300\n",
            "1563/1563 - 68s - loss: 0.0471 - accuracy: 0.9887 - val_loss: 3.6572 - val_accuracy: 0.7088 - 68s/epoch - 43ms/step\n",
            "Epoch 236/300\n",
            "1563/1563 - 68s - loss: 0.0566 - accuracy: 0.9875 - val_loss: 3.4669 - val_accuracy: 0.7051 - 68s/epoch - 43ms/step\n",
            "Epoch 237/300\n",
            "1563/1563 - 69s - loss: 0.0371 - accuracy: 0.9906 - val_loss: 3.8054 - val_accuracy: 0.7115 - 69s/epoch - 44ms/step\n",
            "Epoch 238/300\n",
            "1563/1563 - 68s - loss: 0.0527 - accuracy: 0.9877 - val_loss: 4.0269 - val_accuracy: 0.7069 - 68s/epoch - 44ms/step\n",
            "Epoch 239/300\n",
            "1563/1563 - 68s - loss: 0.0560 - accuracy: 0.9874 - val_loss: 3.8292 - val_accuracy: 0.7035 - 68s/epoch - 43ms/step\n",
            "Epoch 240/300\n",
            "1563/1563 - 70s - loss: 0.0407 - accuracy: 0.9904 - val_loss: 4.0222 - val_accuracy: 0.7063 - 70s/epoch - 45ms/step\n",
            "Epoch 241/300\n",
            "1563/1563 - 68s - loss: 0.0569 - accuracy: 0.9867 - val_loss: 3.6927 - val_accuracy: 0.7078 - 68s/epoch - 44ms/step\n",
            "Epoch 242/300\n",
            "1563/1563 - 70s - loss: 0.0464 - accuracy: 0.9891 - val_loss: 3.8570 - val_accuracy: 0.7071 - 70s/epoch - 44ms/step\n",
            "Epoch 243/300\n",
            "1563/1563 - 68s - loss: 0.0520 - accuracy: 0.9886 - val_loss: 3.9689 - val_accuracy: 0.7052 - 68s/epoch - 44ms/step\n",
            "Epoch 244/300\n",
            "1563/1563 - 68s - loss: 0.0531 - accuracy: 0.9876 - val_loss: 3.6475 - val_accuracy: 0.7092 - 68s/epoch - 44ms/step\n",
            "Epoch 245/300\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-50b5d2910512>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1830\u001b[0m                             \u001b[0mpss_evaluation_shards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pss_evaluation_shards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m                         )\n\u001b[0;32m-> 1832\u001b[0;31m                     val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1833\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2270\u001b[0m                         ):\n\u001b[1;32m   2271\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2272\u001b[0;31m                             logs = test_function_runner.run_step(\n\u001b[0m\u001b[1;32m   2273\u001b[0m                                 \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2274\u001b[0m                                 \u001b[0mdata_handler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   4077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4078\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4079\u001b[0;31m         \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4080\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4081\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    874\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    877\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess the CIFAR-10 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "train_labels, test_labels = to_categorical(train_labels), to_categorical(test_labels)\n",
        "\n",
        "# Build the extended CNN model\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))  # Additional convolutional layer\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))  # Adjusted fully connected layer\n",
        "model.add(layers.Dense(64, activation='relu'))   # Additional fully connected layer with an activation function\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "history = model.fit(train_images, train_labels, epochs=300, validation_data=(test_images, test_labels), verbose=2)\n",
        "end_time = time.time()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nTraining Time:\", round(end_time - start_time, 2), \"seconds\")\n",
        "print(\"Training Loss:\", history.history['loss'][-1])\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ]
    }
  ]
}